{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQOTefYuktS-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evz7--SPWgUY"
      },
      "source": [
        "# PySpark Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDHg3O-WpJz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrPrd7UfWrEP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "e409c4f7-1b62-4f4a-c708-e698dbdd48a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,475 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,532 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,151 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,832 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Fetched 28.5 MB in 6s (4,933 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a1c920ee5d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4b1eb2ea3f0a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyzg30RPXATg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2A1RtI_W-QN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwtUQzFxXBfX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWQ2zdY8XBui"
      },
      "source": [
        "# Agenda\n",
        "\n",
        "\n",
        "  ▶ SaveAsTextFile\n",
        "\n",
        "  ▶ coalesce\n",
        "\n",
        "  ▶ repartition\n",
        "  \n",
        "  ▶ foreach\n",
        "  \n",
        "  ▶ Pipe\n",
        "  \n",
        "  ▶ Join\n",
        "  \n",
        "  ▶ Union\n",
        "  \n",
        "  ▶ mapPartitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W04DLUdiYRU9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_8d-ATYSNL"
      },
      "source": [
        "# SaveAsTextFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Po4943xYVe2"
      },
      "source": [
        "The saveAsTextFile function is used to save the contents of an RDD (Resilient Distributed Dataset) as a text file in a specified directory. Each element in the RDD is written as a separate line in the text file. This is useful for exporting data after processing so that it can be used by other systems or for storage.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "File Format: Saves each element of the RDD as a line in a text file.\n",
        "\n",
        "Directory Structure: It creates a directory with part files inside it (e.g., part-00000), representing different partitions of the RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-uPbGHkYjdq"
      },
      "outputs": [],
      "source": [
        "# Creating a simple RDD\n",
        "sc= spark.sparkContext\n",
        "rdd =sc.parallelize ([\"hello\",\"world\",\"how\",\"are\",\"you\",\"this\",\"is\",\"pyspark\"])\n",
        "rdd1=rdd.coalesce(3)\n",
        "\n",
        "# Saving the RDD to a text file\n",
        "rdd1.saveAsTextFile(\"outputfile126\")\n",
        "\n",
        "# Result: A directory named 'outputfie' is created with part files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Jqf5pTcEhi"
      },
      "source": [
        "##  coalesce\n",
        "\n",
        "function is used to reduce the number of partitions in an RDD. This is particularly useful for optimizing performance when writing data to disk or when you know the dataset is small enough to fit in fewer partitions. It does not perform a full shuffle of the data, making it more efficient than repartition for reducing the number of partitions.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Reduces Partitions: Useful when writing data to reduce the overhead.\n",
        "Efficient: Does not involve a full data shuffle, making it faster for reducing partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzMgVJKecD88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ4iOTSmnQbj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-CpnR0QnQzS"
      },
      "source": [
        "# repartition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyookwuTnR_B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dXV3hNJnSwj"
      },
      "source": [
        "The repartition function in PySpark is a powerful tool used to change the number of partitions in an RDD or DataFrame. Partitions are the logical divisions of data in PySpark, and they play a critical role in how data is distributed across the cluster, affecting performance, parallelism, and resource management.\n",
        "\n",
        "Concept:\n",
        "\n",
        "repartition allows you to increase or decrease the number of partitions in an RDD or DataFrame.\n",
        "\n",
        "When you increase the number of partitions, repartition distributes the data across more partitions to improve parallelism.\n",
        "\n",
        "When you decrease the number of partitions, repartition balances the data by redistributing it, often involving a full shuffle, which ensures that the data is evenly distributed across the new partitions.\n",
        "Full Data Shuffle:\n",
        "\n",
        "Unlike coalesce, which minimizes shuffling, repartition performs a full shuffle of the data across the nodes in the cluster.\n",
        "\n",
        "This is essential when you need to ensure that the data is balanced evenly across partitions, particularly when increasing the number of partitions.\n",
        "When to Use repartition:\n",
        "\n",
        "Increase Parallelism: When your computation is slow because some partitions are much larger than others.\n",
        "\n",
        "Balance Workload: When you need to distribute data more evenly across the cluster to avoid \"skewness.\"\n",
        "\n",
        "Optimize Join Operations: When preparing data for join operations where balanced partitions can improve performance.\n",
        "\n",
        "Prepare Data for Output: When writing data out to disk, repartitioning can help manage the number of output files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCyvvzSdnSmc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEpflzG3o0F1",
        "outputId": "4fe90a02-0d3a-4300-ede3-c4b589a6bac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "par 3\n"
          ]
        }
      ],
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5,6,7,8,8,9],5)\n",
        "#print(\"par\",rdd.getNumPartitions())\n",
        "rdd2=rdd.repartition(3)\n",
        "print(\"par\",rdd2.getNumPartitions())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR2oW4WoqEtR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JguPCsaaqFPI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JntWPAOqF1s"
      },
      "source": [
        "# foreach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwIXY4h-qI4a"
      },
      "source": [
        "The foreach function in PySpark is used to apply a function to each element of an RDD or DataFrame without returning any value. It is particularly useful when you want to perform side effects, such as writing data to an external system, logging, or updating a database. Unlike map, which transforms data and returns a new RDD or DataFrame, foreach is intended for actions that do not produce a result.\n",
        "\n",
        "What is foreach?\n",
        "\n",
        "Concept:\n",
        "\n",
        "The foreach function iterates over each element in the RDD or DataFrame and applies a specified function to it.\n",
        "\n",
        "It is often used for operations that have side effects, like writing data to a database, printing to the console, or updating some external state.\n",
        "Since foreach does not return a new RDD, it is categorized as an action in PySpark (as opposed to a transformation like map).\n",
        "When to Use foreach:\n",
        "\n",
        "Logging: When you want to log information about each element in the RDD for debugging or monitoring.\n",
        "\n",
        "Database Operations: When you need to insert or update records in a database based on the elements in the RDD.\n",
        "\n",
        "External API Calls: When you want to send data to an external service or API for each element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCsv35M2q4YV"
      },
      "outputs": [],
      "source": [
        "rdd =sc.parallelize([1,2,3,4,5,6,7,8,8,9],5)\n",
        "rdd.foreach(lambda x: print(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0zLMkRQrZ3u"
      },
      "outputs": [],
      "source": [
        "def save_to_external_system(data):\n",
        "    print(f\"Saving {data} to an external system...\")\n",
        "\n",
        "rdd=sc.parallelize([(\"user1\",100),(\"user2\",200),(\"user3\",150)])\n",
        "rdd.foreach(save_to_external_system)\n",
        "# for each will process the data without creating a new rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nxZduSur_t7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ_4McTbsK5Q"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7b6OpgzsLQo"
      },
      "source": [
        "# pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KidT4NigsNnk"
      },
      "source": [
        "The pipe function in PySpark allows you to send the output of an RDD through a shell command or an external program, and then collect the processed results back into a new RDD. This feature is particularly useful when you need to integrate PySpark with existing command-line tools or external programs that can process text-based data.\n",
        "\n",
        "What is pipe?\n",
        "\n",
        "\n",
        "The pipe function is used to send the data from each partition of an RDD to a shell command or external program. The command processes the data, and the output from the command is captured as a new RDD.\n",
        "Each partition of the RDD is piped as a separate process to the shell command, meaning that the command operates independently on each partition.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Integrating with Shell Commands: When you have existing scripts or command-line tools that perform specific processing tasks.\n",
        "\n",
        "Interfacing with External Programs: When you need to leverage tools outside of PySpark for tasks such as data formatting, statistical analysis, or other specialized processing.\n",
        "\n",
        "Complex Data Processing: When certain operations are easier or more efficient to implement in a shell command than in PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwYiy3RJszoN",
        "outputId": "0927fc01-294f-4dad-d06e-fa0998eb9f9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1', '2']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd =sc.parallelize([\"hello world\",\"how are you\",\"this is pyspark\"])\n",
        "rdd_pipe1=rdd.pipe(\"wc -l\")\n",
        "rdd_pipe=rdd.pipe(\"tr a-z A-Z\")\n",
        "rdd_pipe.collect()\n",
        "rdd_pipe1.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YM4qVUywhqF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEVQofp8wiWX"
      },
      "source": [
        "# Join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rPQqM_cwmUq"
      },
      "source": [
        "The join operation in PySpark is used to combine two RDDs or DataFrames based on a common key. This is similar to SQL joins, where you combine tables based on a related column. join is particularly useful when working with distributed datasets where you need to merge data from different sources or tables based on shared keys.\n",
        "\n",
        "What is join?\n",
        "\n",
        "\n",
        "The join operation in PySpark is used to merge two RDDs or DataFrames that have the same key type. The result is a new RDD or DataFrame where each key is associated with a tuple containing the values from both datasets.\n",
        "\n",
        "The keys must be unique within each RDD/DataFrame, but they do not need to be unique across the two datasets being joined.\n",
        "\n",
        "Types of Joins:\n",
        "\n",
        "Inner Join: The default join type. Only the keys present in both RDDs or DataFrames are included in the result.\n",
        "\n",
        "Left Outer Join: Includes all keys from the left RDD/DataFrame and their associated values. If the key is not present in the right RDD/DataFrame, the result includes None for those values.\n",
        "\n",
        "Right Outer Join: Includes all keys from the right RDD/DataFrame and their associated values. If the key is not present in the left RDD/DataFrame, the result includes None for those values.\n",
        "\n",
        "Full Outer Join: Includes all keys from both RDDs/DataFrames. Where keys do not match, None is used for missing values.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Combining Related Data: When you have related datasets (e.g., customer information and order data) that you want to combine based on a common key (e.g., customer ID).\n",
        "\n",
        "Data Integration: Merging data from different sources, such as joining sales records with product details.\n",
        "\n",
        "Data Enrichment: Adding additional information to a dataset by joining it with another dataset that contains more detailed or related information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uo83hyrxdhe",
        "outputId": "b30b647c-5915-49dd-f222-7c8c0f8e2aed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('user2', ('hari3', 200)),\n",
              " ('user3', ('hari2', 150)),\n",
              " ('user1', ('hari', 100))]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "rdd_names=sc.parallelize([(\"user1\",\"hari\"),(\"user2\",\"hari3\"),(\"user3\",\"hari2\"),(\"user4\",\"hari1\"),(\"user5\",\"ari\")])\n",
        "rdd_purchases=sc.parallelize([(\"user1\",100),(\"user2\",200),(\"user3\",150)])\n",
        "rdd_names.join(rdd_purchases).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnK2CXETyMoW",
        "outputId": "00fb47ab-64e3-4fc7-8bb6-8cfe317a56c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('user2', ('hari3', 200)),\n",
              " ('user3', ('hari2', 150)),\n",
              " ('user4', ('hari1', None)),\n",
              " ('user1', ('hari', 100)),\n",
              " ('user5', ('ari', None))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "rdd_names.leftOuterJoin(rdd_purchases).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEcoe__lwhgt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swN3rdvrylZD",
        "outputId": "fbdb82bc-f9cc-4a15-fe3a-e3de0af95de4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('user2', ('hari3', 200)),\n",
              " ('user3', ('hari2', 150)),\n",
              " ('user1', ('hari', 100))]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "rdd_names.rightOuterJoin(rdd_purchases).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mbgOzjC9nJYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_names.fullOuterJoin(rdd_purchases).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4-EPKA6A0nU",
        "outputId": "b1f34595-f0e2-4193-e99b-1468cc15885c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('user2', ('hari3', 200)),\n",
              " ('user3', ('hari2', 150)),\n",
              " ('user4', ('hari1', None)),\n",
              " ('user1', ('hari', 100)),\n",
              " ('user5', ('ari', None))]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1,2,3,4,5])\n",
        "rdd1=sc.parallelize([6,7,8,3,2,9,10])\n",
        "rdd_union=rdd1.union(rdd1)\n",
        "rdd_union.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDNwsRqxBCKm",
        "outputId": "c9dee05b-ac13-454c-e38c-c0051ee9f70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 7, 8, 3, 2, 9, 10, 6, 7, 8, 3, 2, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is mapPartitions in PySpark?\n",
        "\n",
        "# mapPartitions is a transformation in PySpark RDD that allows you to process an entire partition at once, instead of processing one record at a time like map does.*`\n",
        "\n",
        "\n",
        "# `It gives you an iterator of a whole partition, and you return another iterator.`"
      ],
      "metadata": {
        "id": "6-3rcMEWB-DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5,6,7,8,8,9],5)\n",
        "\n",
        "def partition_sum(iterator):\n",
        "    yield sum(iterator)\n",
        "\n",
        "rdd2=rdd.mapPartitions(partition_sum)\n",
        "rdd2.collect()\n",
        "#\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tEtM0Y9CSzA",
        "outputId": "12ce5fd0-ddb3-419f-cb89-c450be3eff5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 7, 11, 15, 17]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}