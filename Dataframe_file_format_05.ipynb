{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OIkkfhJwsPr5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0I6AQaWMyV"
      },
      "source": [
        "# Install Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¯ AGENDA\n",
        "\n",
        "\n",
        "Different file formats we can read data from to data fram\n",
        "\n",
        "How to read CSV, JSON, Parquet, Text, ORC\n",
        "\n",
        "Options while reading files\n",
        "â€“ header, inferSchema, delimiter, multiline\n",
        "â€“ schema definition\n",
        "â€“ handling bad records\n",
        "\n",
        "How to write data into files\n",
        "\n",
        "Partitions & performance\n",
        "\n",
        "Hands-on labs\n",
        "\n",
        "Interview questions & alternative methods"
      ],
      "metadata": {
        "id": "98AyHERIsAMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWjsZ1A9WDhm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "c87f482f-1402-4723-f7a4-0b47960f5ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,151 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,472 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,532 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,214 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,830 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,873 kB]\n",
            "Fetched 28.5 MB in 7s (4,368 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a27324eb410>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3943b8626cb0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzX1UEbbWUBj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0wHEbWIWUlf"
      },
      "source": [
        "# Reading and Writing Files to DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R69Bv0HaWsTO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2CZPl00Wvub"
      },
      "source": [
        "\n",
        "\n",
        "### **1. Introduction to Writing Data into Spark DataFrames**\n",
        "\n",
        "In PySpark, the `SparkSession` object provides various methods to read data from external sources into DataFrames. Data can come from different formats like CSV, JSON, Parquet, and more. Each format has its own set of parameters to control how the data is read.\n",
        "\n",
        "### **2. Basic Syntax**\n",
        "\n",
        "```python\n",
        "spark.read.format(\"file_format\").option(\"key\", \"value\").load(\"file_path\")\n",
        "```\n",
        "\n",
        "- `file_format`: The format of the file (e.g., csv, json, parquet, etc.).\n",
        "- `option`: Configuration options (e.g., header, inferSchema).\n",
        "- `load()`: Specifies the path of the file to read.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Reading Data from Different Formats**\n",
        "\n",
        "#### **3.1 Reading CSV Files**\n",
        "\n",
        "CSV (Comma-Separated Values) is a common format for data storage.\n",
        "\n",
        "**Basic Example**:\n",
        "```python\n",
        "# Reading a CSV file\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "df.show()\n",
        "```\n",
        "- `header=True`: Indicates that the first row of the CSV contains column names.\n",
        "- `inferSchema=True`: Automatically infers the schema (data types) of the columns.\n",
        "\n",
        "**With Additional Options**:\n",
        "```python\n",
        "df = spark.read.option(\"delimiter\", \";\").csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "df.show()\n",
        "```\n",
        "- `delimiter=\";\"`: Specifies a custom delimiter (e.g., semicolon) instead of the default comma.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3.2 Reading JSON Files**\n",
        "\n",
        "JSON (JavaScript Object Notation) is a lightweight data-interchange format.\n",
        "\n",
        "**Basic Example**:\n",
        "```python\n",
        "# Reading a JSON file\n",
        "df = spark.read.json(\"path/to/file.json\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "**With Schema Inference**:\n",
        "```python\n",
        "df = spark.read.option(\"multiline\", True).json(\"path/to/file.json\")\n",
        "df.show()\n",
        "```\n",
        "- `multiline=True`: Used when the JSON file contains data spanning multiple lines.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3.3 Reading Parquet Files**\n",
        "\n",
        "Parquet is a columnar storage file format that provides efficient data compression.\n",
        "\n",
        "**Basic Example**:\n",
        "```python\n",
        "# Reading a Parquet file\n",
        "df = spark.read.parquet(\"path/to/file.parquet\")\n",
        "df.show()\n",
        "```\n",
        "- Parquet format automatically preserves the schema and data types, so there's no need for `header` or `inferSchema` options.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3.4 Reading Text Files**\n",
        "\n",
        "Text files contain plain text, where each line represents one record.\n",
        "\n",
        "**Basic Example**:\n",
        "```python\n",
        "# Reading a text file\n",
        "df = spark.read.text(\"path/to/file.txt\")\n",
        "df.show()\n",
        "```\n",
        "- This reads the file as a single column DataFrame with each line stored as a string.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3.5 Reading ORC Files**\n",
        "\n",
        "ORC (Optimized Row Columnar) is a format used for optimized storage.\n",
        "\n",
        "**Basic Example**:\n",
        "```python\n",
        "# Reading an ORC file\n",
        "df = spark.read.orc(\"path/to/file.orc\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Writing DataFrames to Files**\n",
        "\n",
        "You can also write Spark DataFrames to different file formats using the `write` method.\n",
        "\n",
        "#### **4.1 Writing CSV Files**\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Writing DataFrame to CSV\n",
        "df.write.csv(\"path/to/output_directory\", header=True)\n",
        "```\n",
        "\n",
        "#### **4.2 Writing JSON Files**\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Writing DataFrame to JSON\n",
        "df.write.json(\"path/to/output_directory\")\n",
        "```\n",
        "\n",
        "#### **4.3 Writing Parquet Files**\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Writing DataFrame to Parquet\n",
        "df.write.parquet(\"path/to/output_directory\")\n",
        "```\n",
        "\n",
        "#### **4.4 Writing Text Files**\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Writing DataFrame to Text\n",
        "df.write.text(\"path/to/output_directory\")\n",
        "```\n",
        "\n",
        "#### **4.5 Writing ORC Files**\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "# Writing DataFrame to ORC\n",
        "df.write.orc(\"path/to/output_directory\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Handling Large Files with PySpark**\n",
        "\n",
        "For large datasets, PySpark can split the data into multiple partitions for better performance. By default, PySpark reads files in a distributed manner.\n",
        "\n",
        "#### **Example**:\n",
        "```python\n",
        "# Specifying the number of partitions while reading\n",
        "df = spark.read.csv(\"path/to/large_file.csv\", header=True, inferSchema=True).repartition(5)\n",
        "```\n",
        "- `.repartition(5)`: This will split the file into 5 partitions for parallel processing.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Additional Options for Reading Files**\n",
        "\n",
        "1. **Schema Definition**: You can define your own schema instead of relying on schema inference.\n",
        "\n",
        "   **Example**:\n",
        "   ```python\n",
        "   from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "   \n",
        "   schema = StructType([\n",
        "       StructField(\"name\", StringType(), True),\n",
        "       StructField(\"age\", IntegerType(), True),\n",
        "       StructField(\"salary\", IntegerType(), True)\n",
        "   ])\n",
        "   \n",
        "   df = spark.read.schema(schema).csv(\"path/to/file.csv\", header=True)\n",
        "   df.show()\n",
        "   ```\n",
        "\n",
        "2. **Handling Bad Records**: Use the `mode` option to specify how bad records (corrupt or malformed) are handled.\n",
        "   - **Options**: `PERMISSIVE` (default), `DROPMALFORMED`, `FAILFAST`\n",
        "   \n",
        "   **Example**:\n",
        "   ```python\n",
        "   df = spark.read.option(\"mode\", \"DROPMALFORMED\").csv(\"path/to/file.csv\", header=True)\n",
        "   df.show()\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Practical Exercise for Students**\n",
        "\n",
        "**Exercise**: Ask students to read a CSV file, filter the data based on some conditions, and write the filtered DataFrame to a Parquet file.\n",
        "\n",
        "```python\n",
        "# Reading CSV file\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Filtering the data (e.g., filtering rows where age is greater than 30)\n",
        "filtered_df = df.filter(df['age'] > 30)\n",
        "\n",
        "# Writing the filtered DataFrame to Parquet format\n",
        "filtered_df.write.parquet(\"path/to/output_directory\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKd1rXe1Y0JC",
        "outputId": "44c1d427-a62b-4ecc-a46a-016c9a565c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+---+\n",
            "|First_Name|Last_Name|       DOB|Age|\n",
            "+----------+---------+----------+---+\n",
            "|   Michael|  Johnson|1998-05-14| 21|\n",
            "|   Michael|    Jones|1987-02-15| 29|\n",
            "|      Sara|      Doe|1985-06-15| 25|\n",
            "|   Michael|   Garcia|2002-12-05| 28|\n",
            "|    Olivia|     King|1984-03-26| 21|\n",
            "|    Sophia|     King|1987-04-12| 26|\n",
            "|    Olivia|    Allen|1977-04-22| 20|\n",
            "|    Robert|   Walker|1984-04-09| 24|\n",
            "|    Olivia|     Hall|1980-01-03| 21|\n",
            "+----------+---------+----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.csv('/content/person_data.csv',header=True)\n",
        "filter_df=df.filter(df['age']<30)\n",
        "#filter1_df.write.parquet('/content/filtered1.parquet')\n",
        "filter_output1 = spark.read.parquet(\"/content/filtered1.parquet/part-00000-28ca61f7-ea3e-4725-a875-0b1b0e5b1ae3-c000.snappy.parquet\")\n",
        "filter_output1.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the csv file to json\n",
        "Break"
      ],
      "metadata": {
        "id": "eox_OgtKs6EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnBqgoAkazms"
      },
      "outputs": [],
      "source": [
        "#df.write.parquet(\"/content/person_data.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df33=spark.read.parquet(\"/content/person_data.parquet/part-00000-317c95fe-5c78-4429-bbbf-794e8236335f-c000.snappy.parquet\")\n",
        "df33.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "You5obstrKXf",
        "outputId": "d19b5cc3-a4ed-4fc4-e4ef-e4b9f5dc246f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(First_Name='Michael', Last_Name='Garcia', DOB='1990-06-12', Age='48'),\n",
              " Row(First_Name='Lucy', Last_Name='Jones', DOB='2018-10-08', Age='31'),\n",
              " Row(First_Name='Sara', Last_Name='Williams', DOB='2016-08-24', Age='47'),\n",
              " Row(First_Name='Michael', Last_Name='Johnson', DOB='1998-05-14', Age='21'),\n",
              " Row(First_Name='Michael', Last_Name='Jones', DOB='1987-02-15', Age='29'),\n",
              " Row(First_Name='Sara', Last_Name='Doe', DOB='1985-06-15', Age='25'),\n",
              " Row(First_Name='Emily', Last_Name='Johnson', DOB='2014-01-05', Age='33'),\n",
              " Row(First_Name='Jane', Last_Name='Williams', DOB='1998-01-28', Age='40'),\n",
              " Row(First_Name='Michael', Last_Name='Garcia', DOB='2002-12-05', Age='28'),\n",
              " Row(First_Name='Michael', Last_Name='Brown', DOB='2011-09-23', Age='40'),\n",
              " Row(First_Name='John', Last_Name='Johnson', DOB='1999-07-18', Age='35'),\n",
              " Row(First_Name='Jane', Last_Name='Martinez', DOB='2001-03-06', Age='55'),\n",
              " Row(First_Name='Emily', Last_Name='Brown', DOB='1997-10-07', Age='41'),\n",
              " Row(First_Name='Emily', Last_Name='Doe', DOB='2016-10-17', Age='51'),\n",
              " Row(First_Name='Jane', Last_Name='Doe', DOB='1982-03-07', Age='35'),\n",
              " Row(First_Name='John', Last_Name='Garcia', DOB='2001-07-30', Age='30'),\n",
              " Row(First_Name='Anna', Last_Name='Smith', DOB='2007-09-26', Age='45'),\n",
              " Row(First_Name='Will', Last_Name='Williams', DOB='1986-03-29', Age='53'),\n",
              " Row(First_Name='Tom', Last_Name='Brown', DOB='2005-10-02', Age='55'),\n",
              " Row(First_Name='Michael', Last_Name='Doe', DOB='2000-02-22', Age='49'),\n",
              " Row(First_Name='Olivia', Last_Name='King', DOB='1984-03-26', Age='21'),\n",
              " Row(First_Name='Emma', Last_Name='Walker', DOB='1976-10-17', Age='42'),\n",
              " Row(First_Name='Sophia', Last_Name='King', DOB='1987-04-12', Age='26'),\n",
              " Row(First_Name='Emma', Last_Name='Green', DOB='2011-12-04', Age='58'),\n",
              " Row(First_Name='Olivia', Last_Name='Allen', DOB='1977-04-22', Age='20'),\n",
              " Row(First_Name='Olivia', Last_Name='Allen', DOB='2006-03-11', Age='57'),\n",
              " Row(First_Name='Emma', Last_Name='Lewis', DOB='2021-06-25', Age='59'),\n",
              " Row(First_Name='Noah', Last_Name='Walker', DOB='2017-09-10', Age='51'),\n",
              " Row(First_Name='Robert', Last_Name='Walker', DOB='1984-04-09', Age='24'),\n",
              " Row(First_Name='Olivia', Last_Name='Hall', DOB='1980-01-03', Age='21')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df31=spark.read.parquet(\"/content/person_data.parquet/part-00000-317c95fe-5c78-4429-bbbf-794e8236335f-c000.snappy\")\n",
        "df31.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "yQn3KioWqdzc",
        "outputId": "d232da9a-9388-4114-c56e-a91163c70706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/person_data.parquet/part-00000-317c95fe-5c78-4429-bbbf-794e8236335f-c000.snappy.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3341649304.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf31\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/person_data.parquet/part-00000-317c95fe-5c78-4429-bbbf-794e8236335f-c000.snappy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf31\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/person_data.parquet/part-00000-317c95fe-5c78-4429-bbbf-794e8236335f-c000.snappy."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "96ghkW8Rb-a1"
      },
      "outputs": [],
      "source": [
        "df4.read.parquet(\"/content/emp1.parqu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "What is the primary storage format used\n",
        "in Databricks for structured data?\n",
        "\n",
        "-> csv\n",
        "-> parquet\n",
        "->json\n",
        "->"
      ],
      "metadata": {
        "id": "25SGZgkm5kWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ". Which PySpark method is used to read a CSV file with headers into a DataFrame?\n",
        "a) spark.read.csv(\"file.csv\", header=True)\n",
        "b) spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file.csv\")\n",
        "c) spark.read.option(\"header\", \"true\").csv(\"file.csv\")\n",
        "d) All of the above"
      ],
      "metadata": {
        "id": "nxViwg386cw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. What is the correct way to register a DataFrame\n",
        " as a temporary view in PySpark?\n",
        "\n",
        "a) df.createTempView(\"my_view\")\n",
        "b) df.createOrReplaceTempView(\"my_view\")\n",
        "c) df.registerTempTable(\"my_view\")\n",
        "d) Both a and b are correct"
      ],
      "metadata": {
        "id": "VuTiV3NT6yb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Which Spark SQL function would you use to parse a JSON string column?\n",
        "a) json_parse()\n",
        "b) from_json()\n",
        "c) parse_json()\n",
        "d) json_extract()"
      ],
      "metadata": {
        "id": "QnUtIf687EY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. In PySpark, how do you select specific columns from a DataFrame?\n",
        "a) df.select(\"col1\", \"col2\")\n",
        "b) df.select(df.col1, df.col2)\n",
        "c) df.select(F.col(\"col1\"), F.col(\"col2\"))\n",
        "d) All of the above"
      ],
      "metadata": {
        "id": "VTWoVkj_7QY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"year\") \\\n",
        ".parquet(\"/path/to/table\")\n",
        "\n",
        "df.write.mode(\"overwrite\").partitionBy(\"year\").parquet(\"/path/to/table\")\n",
        "a) Writes data in append mode with year partitioning\n",
        "b) Writes data in overwrite mode with year partitioning\n",
        "c) Creates a new table partitioned by year\n",
        "d) Updates existing partitions for the specified year\n"
      ],
      "metadata": {
        "id": "kwkhV1BL7mZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ". Which SQL command creates a managed table in Databricks?\n",
        "a) CREATE TABLE table_name USING DELTA LOCATION '/path'\n",
        "b) CREATE TABLE table_name (col1 STRING, col2 INT) USING DELTA\n",
        "c) CREATE OR REPLACE TABLE table_name AS SELECT * FROM source\n",
        "d) Both b and c are correct"
      ],
      "metadata": {
        "id": "pPQ9UKsr75ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Which PySpark transformation is used to remove duplicate rows?\n",
        "a) df.distinct()\n",
        "b) df.dropDuplicates()\n",
        "c) df.drop_duplicates()\n",
        "d) Both a and b are correct"
      ],
      "metadata": {
        "id": "PZjtt6ZN8COf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Which PySpark method is used to filter rows in a DataFrame?\n",
        "a) df.filter(col(\"age\") > 25)\n",
        "b) df.where(col(\"age\") > 25)\n",
        "c) df.filter(\"age > 25\")\n",
        "d) All of the above"
      ],
      "metadata": {
        "id": "7-HD4diw8rty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "df.withColumn(\"status\", when(col(\"age\") >= 18, \"Adult\").otherwise(\"Minor\"))\n",
        "\n",
        "\n",
        "a) Filters rows where age >= 18\n",
        "b) Creates a new column \"status\" with conditional values\n",
        "c) Updates the \"age\" column with status values\n",
        "d) Groups data by age ranges"
      ],
      "metadata": {
        "id": "9iCTtrnW9Fea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " What does the collect() action do in PySpark?\n",
        "a) Collects all rows from the DataFrame to the driver\n",
        "b) Groups rows by a specified column\n",
        "c) Counts the number of rows in the DataFrame\n",
        "d) Removes duplicate rows from the DataFrame\n",
        "\n",
        "list of rows\n"
      ],
      "metadata": {
        "id": "Tnp2b8yi9grN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. In Spark SQL, which function is used to get the current date?\n",
        "a) current_date()\n",
        "b) now()\n",
        "c) today()\n",
        "d) getdate()"
      ],
      "metadata": {
        "id": "zm0_Rn3L9x-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the correct way to add a new column with a constant value to a DataFrame?\n",
        "a) df.withColumn(\"new_col\", \"constant_value\")\n",
        "b) df.withColumn(\"new_col\", lit(\"constant_value\"))\n",
        "c) df.select(\"*\", lit(\"constant_value\").alias(\"new_col\"))\n",
        "d) Both b and c are correct"
      ],
      "metadata": {
        "id": "lbcolA24-CMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Which window function would you use to assign row numbers to each row within a partition?\n",
        "a) rank()\n",
        "b) dense_rank()\n",
        "c) row_number()\n",
        "d) ntile()"
      ],
      "metadata": {
        "id": "4zgcmWFq-Uqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").agg(\n",
        "    count(\"*\").alias(\"employee_count\"),\n",
        "    avg(\"salary\").alias(\"avg_salary\")\n",
        ")\n",
        "\n",
        "a) Filters employees by department\n",
        "b) Groups by department and calculates count and average salary\n",
        "c) Sorts employees by department and salary\n",
        "d) Creates a pivot table"
      ],
      "metadata": {
        "id": "UA-WC0D8-fkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.format(\"csv\").option(\"header\", True).load(\"/content/sample_data.csv\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhcYjGrctul0",
        "outputId": "d38e2113-4e27-46f2-8312-148f7539188a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+---------+\n",
            "| id|   name|age|     city|\n",
            "+---+-------+---+---------+\n",
            "|  1|  Alice| 25|Hyderabad|\n",
            "|  2|    Bob| 30|Bangalore|\n",
            "|  3|Charlie| 28|  Chennai|\n",
            "+---+-------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",True).load(\"/content/sample_data.csv\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxXEIG3FvZHs",
        "outputId": "6f78a321-137b-4368-e554-3628d0797294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+---------+\n",
            "| id|   name|age|     city|\n",
            "+---+-------+---+---------+\n",
            "|  1|  Alice| 25|Hyderabad|\n",
            "|  2|    Bob| 30|Bangalore|\n",
            "|  3|Charlie| 28|  Chennai|\n",
            "+---+-------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlKutkFyv0Mi",
        "outputId": "b0f9a8dc-d1df-4e33-a958-2e319c97639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(\"/content/sample_data.parquet\")"
      ],
      "metadata": {
        "id": "y9npBZaOwUpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pFjFBa6Bwfx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.format(\"parquet\").load(\"/content/sample_data.parquet\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzum462bwmBw",
        "outputId": "411e6d4e-e1b5-438a-a5a2-5d82e493e15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+---------+\n",
            "| id|   name|age|     city|\n",
            "+---+-------+---+---------+\n",
            "|  1|  Alice| 25|Hyderabad|\n",
            "|  2|    Bob| 30|Bangalore|\n",
            "|  3|Charlie| 28|  Chennai|\n",
            "+---+-------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.format(\"json\").load(\"/content/sample_data.json\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddF8KxhlxAm7",
        "outputId": "b7fa3abd-aec6-4ae3-c3e9-f8297fe50d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---+-------+\n",
            "|age|     city| id|   name|\n",
            "+---+---------+---+-------+\n",
            "| 25|Hyderabad|  1|  Alice|\n",
            "| 30|Bangalore|  2|    Bob|\n",
            "| 28|  Chennai|  3|Charlie|\n",
            "+---+---------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.json(\"/content/sample_data.json\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5sWYEcxxKVJ",
        "outputId": "27d4ccb5-4fe5-4a4b-8520-65f6f56f8b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---+-------+\n",
            "|age|     city| id|   name|\n",
            "+---+---------+---+-------+\n",
            "| 25|Hyderabad|  1|  Alice|\n",
            "| 30|Bangalore|  2|    Bob|\n",
            "| 28|  Chennai|  3|Charlie|\n",
            "+---+---------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.csv(\"/content/sample_data1.csv\")"
      ],
      "metadata": {
        "id": "0j-3bohAxU5b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}