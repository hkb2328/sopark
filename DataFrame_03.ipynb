{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Pyspark"
      ],
      "metadata": {
        "id": "2Cb-MVn4e_vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9e0VvXYTfD5n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8G386b4fEty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxv7w_2y2bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "2cabe421-f455-48ba-e431-fc08068ee712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "44 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b30c7946ab0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://868cf18d1ce7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HsPHcfNewwg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ytdale3Te_D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KauI9m86fV6L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TzkINkEfXHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PlOJz5sHfXoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A-Ez_UOb9xoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Column Data"
      ],
      "metadata": {
        "id": "CyTe-J8IfYGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here is a breakdown of the PySpark functions from the import statement, explained as points:\n",
        "\n",
        "### String Manipulation Functions\n",
        "1. **`col`**: Refers to columns within a DataFrame. Used to apply operations on columns.\n",
        "2. **`lower`**: Converts string columns to lowercase.\n",
        "3. **`upper`**: Converts string columns to uppercase.\n",
        "4. **`length`**: Returns the length of a string column.\n",
        "5. **`substring`**: Extracts a substring from a string column, based on start and length.\n",
        "6. **`split`**: Splits a string into an array of substrings based on a delimiter.\n",
        "7. **`trim`**: Removes leading and trailing whitespace from a string.\n",
        "8. **`ltrim`**: Removes leading whitespace from a string.\n",
        "9. **`rtrim`**: Removes trailing whitespace from a string.\n",
        "10. **`lpad`**: Pads the left side of a string column to a specified length with a padding character.\n",
        "11. **`rpad`**: Pads the right side of a string column to a specified length with a padding character.\n",
        "12. **`concat`**: Concatenates two or more columns into a single column.\n",
        "13. **`concat_ws`**: Concatenates two or more columns with a separator (e.g., `-` or `,`).\n",
        "\n",
        "### Date Manipulation Functions\n",
        "14. **`current_date`**: Returns the current date.\n",
        "15. **`current_timestamp`**: Returns the current timestamp.\n",
        "16. **`date_add`**: Adds a specified number of days to a date column.\n",
        "17. **`date_sub`**: Subtracts a specified number of days from a date column.\n",
        "18. **`datediff`**: Returns the difference in days between two dates.\n",
        "19. **`months_between`**: Returns the number of months between two dates.\n",
        "20. **`add_months`**: Adds a specified number of months to a date column.\n",
        "21. **`next_day`**: Returns the next specified day of the week after a given date.\n",
        "22. **`last_day`**: Returns the last day of the month for a given date.\n",
        "23. **`trunc`**: Truncates a date to the specified unit (e.g., year, month).\n",
        "24. **`date_trunc`**: Truncates a timestamp to the specified unit (e.g., hour, day).\n",
        "25. **`date_format`**: Formats a date according to a specified pattern (e.g., `yyyy/MM/dd`).\n",
        "26. **`dayofyear`**: Extracts the day of the year from a date.\n",
        "27. **`dayofmonth`**: Extracts the day of the month from a date.\n",
        "28. **`dayofweek`**: Extracts the day of the week from a date.\n",
        "29. **`year`**: Extracts the year from a date.\n",
        "30. **`month`**: Extracts the month from a date.\n",
        "\n",
        "### Aggregate Functions\n",
        "31. **`count`**: Returns the number of rows in a group.\n",
        "32. **`countDistinct`**: Returns the number of distinct rows in a group.\n",
        "33. **`sum`**: Returns the sum of values in a column.\n",
        "34. **`avg`**: Returns the average of values in a column.\n",
        "35. **`min`**: Returns the minimum value of a column.\n",
        "36. **`max`**: Returns the maximum value of a column.\n",
        "\n",
        "### Conditional and Type Functions\n",
        "37. **`when`**: Used for conditional expressions (similar to `CASE WHEN` in SQL).\n",
        "38. **`lit`**: Creates a literal value in a DataFrame column.\n",
        "\n",
        "### Special Types - ARRAY, MAP, STRUCT, CAST\n",
        "39. **`array`**: Creates an array column from multiple columns.\n",
        "40. **`map_from_arrays`**: Creates a map (key-value pairs) from two arrays.\n",
        "41. **`struct`**: Combines multiple columns into a single struct column (nested columns).\n",
        "42. **`cast`**: Casts the data type of a column to another type (e.g., `int` to `string`).\n",
        "\n",
        "These points will help explain each function step by step to your students."
      ],
      "metadata": {
        "id": "hGOustQtkAHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, upper, length, substring, split, trim, ltrim, rtrim, \\\n",
        "    lpad, rpad, concat, concat_ws, current_date, current_timestamp, date_add, date_sub, datediff, \\\n",
        "    months_between, add_months, next_day, last_day, trunc, date_trunc, date_format, dayofyear, \\\n",
        "    dayofmonth, dayofweek, year, month, count, countDistinct, sum, avg, min, max, when, lit, \\\n",
        "    array, map_from_arrays, struct, cast"
      ],
      "metadata": {
        "id": "e8_DKaJDjCJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.csv(\"/content/sales_data_1000.csv\",header=True,inferSchema=True)\n",
        "df.show()\n",
        "\n",
        "#inferSchema=True -> It automatically infer column data types\n",
        "#encoding=\"UTF-8\"\n",
        "#nullValue =\"NA\" -> treat NA as null\n",
        "#sep=\",\" ->Filed Delimiter ,Default\n",
        "\n",
        "#df2=df.withColumn(\"category_lenght\", len(col(\"category\")))\n",
        "#df2.show()\n",
        "\n",
        "#df3=df.withColumn(\"date_add\",date_add(col(\"sales_date\"),1)).show()\n",
        "#df3.show()\n",
        "df3=df.withColumn(\"date_sub\",date_sub(col(\"sales_date\"),10))\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "CI6-ymm3rbRl",
        "outputId": "aff12ede-88dc-4dd7-a6fe-1faa6e767edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------+-----------+--------+--------+---------+----------+\n",
            "|transaction_id|customer_id|   product|   category|quantity|   price|     city| sale_date|\n",
            "+--------------+-----------+----------+-----------+--------+--------+---------+----------+\n",
            "|             1|        178|    Tablet|Electronics|       6|29546.59|Hyderabad|2024-11-06|\n",
            "|             2|        126|    Laptop|Electronics|       9|51856.81|    Delhi|2024-08-30|\n",
            "|             3|        177|   Charger|Accessories|       2|60725.66|    Delhi|2024-05-26|\n",
            "|             4|        271|    Camera|Electronics|       7|80424.94|   Mumbai|2024-09-19|\n",
            "|             5|        281|     Watch|  Wearables|       3|15546.03|   Mumbai|2024-01-06|\n",
            "|             6|         68|    Laptop|Electronics|      10| 25425.0|    Delhi|2024-11-29|\n",
            "|             7|         64|    Laptop|Electronics|       2|24791.55|     Pune|2024-05-05|\n",
            "|             8|        179|    Laptop|Electronics|       5| 89351.3|  Chennai|2024-02-09|\n",
            "|             9|        287|    Camera|Electronics|       3|70863.98|     Pune|2024-04-19|\n",
            "|            10|         53|     Watch|  Wearables|      10|81575.19|Bangalore|2024-02-18|\n",
            "|            11|        292|    Laptop|Electronics|       5|47951.08|Hyderabad|2024-03-18|\n",
            "|            12|         90|    Laptop|Electronics|      10|54688.26|Bangalore|2024-02-28|\n",
            "|            13|        223|   Charger|Accessories|      10|55681.38|Hyderabad|2024-05-21|\n",
            "|            14|         68|     Watch|  Wearables|       7|56697.15|     Pune|2024-06-14|\n",
            "|            15|        291|    Mobile|Electronics|       2|20756.91|Bangalore|2024-07-13|\n",
            "|            16|         52|   Charger|Accessories|       4|70921.12|   Mumbai|2024-09-12|\n",
            "|            17|         36|    Laptop|Electronics|       2|  3551.3|Hyderabad|2024-05-18|\n",
            "|            18|         19|    Camera|Electronics|       4|46796.77|Bangalore|2024-06-21|\n",
            "|            19|         32|Headphones|Accessories|       4|45414.78|Hyderabad|2024-02-18|\n",
            "|            20|        232|   Charger|Accessories|       3|71519.26|     Pune|2024-06-17|\n",
            "+--------------+-----------+----------+-----------+--------+--------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sales_date` cannot be resolved. Did you mean one of the following? [`sale_date`, `category`, `product`, `city`, `price`].;\n'Project [transaction_id#488, customer_id#489, product#490, category#491, quantity#492, price#493, city#494, sale_date#495, date_sub('sales_date, 10) AS date_sub#546]\n+- Relation [transaction_id#488,customer_id#489,product#490,category#491,quantity#492,price#493,city#494,sale_date#495] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3117285027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#df3=df.withColumn(\"date_add\",date_add(col(\"sales_date\"),1)).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#df3.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date_sub\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sales_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sales_date` cannot be resolved. Did you mean one of the following? [`sale_date`, `category`, `product`, `city`, `price`].;\n'Project [transaction_id#488, customer_id#489, product#490, category#491, quantity#492, price#493, city#494, sale_date#495, date_sub('sales_date, 10) AS date_sub#546]\n+- Relation [transaction_id#488,customer_id#489,product#490,category#491,quantity#492,price#493,city#494,sale_date#495] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuzpWRisP0TA",
        "outputId": "5edb4654-e919-4075-e8c8-cff1af0785ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- transaction_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- sale_date: date (nullable = true)\n",
            "\n",
            "+--------------+-----------+-------+-----------+--------+--------+---------+----------+\n",
            "|transaction_id|customer_id|product|   category|quantity|   price|     city| sale_date|\n",
            "+--------------+-----------+-------+-----------+--------+--------+---------+----------+\n",
            "|             1|        178| Tablet|Electronics|       6|29546.59|Hyderabad|2024-11-06|\n",
            "|             2|        126| Laptop|Electronics|       9|51856.81|    Delhi|2024-08-30|\n",
            "|             3|        177|Charger|Accessories|       2|60725.66|    Delhi|2024-05-26|\n",
            "|             4|        271| Camera|Electronics|       7|80424.94|   Mumbai|2024-09-19|\n",
            "|             5|        281|  Watch|  Wearables|       3|15546.03|   Mumbai|2024-01-06|\n",
            "+--------------+-----------+-------+-----------+--------+--------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with column is uses to add new col to the existing data frame\n",
        "or update the existing columns or replace the existing columns with new values"
      ],
      "metadata": {
        "id": "ofpRD_5YQzpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.csv(\"/content/person_data.csv\",header=True,inferSchema=True)\n",
        "df1=df.select(col(\"First_Name\"))\n",
        "df1.show(5)\n",
        "df2=df.withColumn(\"Upper_case_name\", upper(col(\"First_Name\")))\n",
        "df3=df2.withColumn(\"country\",lit(\"India\"))\n",
        "df4=df3.withColumn(\"age+5\",col(\"Age\")+5)\n",
        "df5=df4.withColumn(\"Birth_year\",year(\"DOB\")) \\\n",
        "      .withColumn(\"Birth_month\",month(\"DOB\")) \\\n",
        "      .withColumn(\"Birth_day\",dayofmonth(\"DOB\"))\n",
        "\n",
        "df_flag =df4.withColumn(\n",
        "    \"Is_Adult\",\n",
        "    when(col(\"Age\")>=25,lit(\"Y\")).otherwise(lit(\"N\"))\n",
        ")\n",
        "df_flag.show()\n",
        "\n",
        "#Extract the year into a new column called JoinYear.\n",
        "\n"
      ],
      "metadata": {
        "id": "BTD1-eDnQWyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a =\"i am learning pyspark\"\n",
        "len(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkDltkPWaZ1w",
        "outputId": "8b66c2aa-04d7-4277-f932-a2fb95067a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.select(col(\"First_Name\"))\n",
        "df2=df1.withColumn(\"First_Name,Lower\", lower(col(\"First_Name\")))\n",
        "df2.withColumn(\"First_Name_upper\", upper(col(\"First_Name\")))\n",
        "df4=df2.withColumn(\"first_name_length\", length(col(\"First_Name\")))\n",
        "df4.show()"
      ],
      "metadata": {
        "id": "6H23CajOs0Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the substring\n",
        "\n",
        "df.withColumn(\"sub_name\",substring(col(\"First_name\"),2,4)).show()\n"
      ],
      "metadata": {
        "id": "znzFRIqAuNVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "\n",
        "df.withColumn(\"split_name\",split(col(\"DOB\"),\"-\")).show()"
      ],
      "metadata": {
        "id": "pX6e8bFZvd4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trim,ltrim,rtrim\n",
        "#df.withColumn(\"trim_name\",trim(col(\"First_Name\"))).show()\n",
        "df.withColumn(\"ltrim_name\",ltrim(col(\"First_Name\"))).show()\n",
        "#df.withColumn(\"rtrim_name\",rtrim(col(\"First_Name\"))).show()"
      ],
      "metadata": {
        "id": "O2AbYhBsvxvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lpad,rpad\n",
        "df.withColumn(\"lpad_name\",lpad(col(\"First_Name\"),15,\"0\")).show()\n",
        "df.withColumn(\"lpad_name\",rpad(col(\"First_Name\"),15,\"|\")).show()\n"
      ],
      "metadata": {
        "id": "A1EpxRIcwR3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concat\n",
        "#df.withColumn(\"concat_name\",concat_ws(\"|\",col(\"First_Name\"),col(\"Last_Name\"))).show()\n",
        "df.withColumn(\"concat1_name\",concat(col(\"First_Name\"),lit(\"-\"),col(\"Last_Name\"))).show()\n",
        "#"
      ],
      "metadata": {
        "id": "IlyWf1PvxPYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Date\n",
        "df.withColumn(\"current_date\",current_date()).show()\n",
        "df.withColumn(\"current_timestamp\",current_timestamp()).show()\n"
      ],
      "metadata": {
        "id": "161lexIByeeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#date_add\n",
        "df.withColumn(\"date_add\",date_add(col(\"DOB\"),1)).show()\n",
        "#df.withColumn(\"date_sub\",date_sub(col(\"DOB\"),10)).show()"
      ],
      "metadata": {
        "id": "BIP35vcQy9nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#months_between,add months\n",
        "\n",
        "df.withColumn(\"months_between\",months_between(current_date(),col(\"DOB\"))).show()\n",
        "df=df.withColumn(\"add_months\",add_months(col(\"DOB\"),3)).show()\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "Nb1c5ClLzvtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nextday,lastday\n",
        "\n",
        "df.withColumn(\"next_day\",next_day(col(\"DOB\"),\"Sunday\")).show()\n",
        "df.withColumn(\"last_day\",last_day(col(\"DOB\"))).show()\n"
      ],
      "metadata": {
        "id": "Yrj5jaNA1YVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dayofYear dayofmonth\n",
        "\n",
        "df.withColumn(\"dayofYear\",dayofyear(col(\"DOB\"))).show()\n",
        "df.withColumn(\"dayofmonth\",dayofmonth(col(\"DOB\"))).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "RL62NFMp10Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dayofweek year\n",
        "df.withColumn(\"dayofweek\",dayofweek(col(\"DOB\"))).show()\n",
        "df.withColumn(\"year\",year(col(\"DOB\"))).show()\n",
        "df.withColumn(\"month\",month(col(\"DOB\"))).show()"
      ],
      "metadata": {
        "id": "agRHpZ2z2NLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"First_Name\").agg(count(\"First_Name\").alias(\"count\")).show()\n",
        "df.groupBy(\"Last_Name\").agg(countDistinct(\"First_Name\").alias(\"dis_count\")).show()"
      ],
      "metadata": {
        "id": "7doMbkjD3kUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUM MAX AVG\n",
        "df.agg(sum(\"age\")).alias(\"total_age\").show()\n",
        "df.agg(max(\"age\")).alias(\"max_age\").show()\n",
        "df.agg(avg(\"age\")).alias(\"avg_age\").show()\n",
        "df.agg(min(\"age\")).alias(\"min_age\").show()\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "ETiMFYBj4E-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4=df.withColumn(\"age_group\",when(col(\"Age\")<30,\"young\").otherwise(\"old\"))\n",
        "df4.show()"
      ],
      "metadata": {
        "id": "MtrDhLx1sjs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when\n",
        "\n",
        "df4 =df.withColumn(\"age_group\",when(col(\"Age\")<30,\"young\").otherwise(\"old\")).show()\n"
      ],
      "metadata": {
        "id": "q-Xuq4JQ42kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"array_col\",array(col(\"First_Name\"),col(\"Last_Name\"))).show()\n",
        "df.withColumn(\"map_col\",map_from_arrays(array(lit(\"first_name\"),lit(\"last_name\")),array(col(\"First_Name\"),col(\"Last_Name\")))).show()\n",
        "df.withColumn(\"struct_col\",struct(col(\"First_Name\"),col(\"Last_Name\"))).show()\n",
        "#"
      ],
      "metadata": {
        "id": "fRHSnWHW5il6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\"First_name_lower\",lower(col(\"First_Name\")))\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "WPTeJKgUtk7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\"First_name_lower\",lower(col(\"First_Name\")))\n",
        "\n",
        "cols = df2.columns\n",
        "\n",
        "df3=df2.select(cols[0],\"First_name_lower\",*[c for c in cols[2:] if c != \"First_name_lower\"])\n",
        "\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "Wd6f0ABFvJAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}