{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "U4hnCmqC-iDz",
        "mJZgO1Ew-Xrn",
        "0iyog2usB5jD",
        "on70yaimCqOe",
        "Ep7XJvc4CxQs",
        "TlgKZf5RGorM",
        "UJdcWYjEHw2g",
        "8QDg5Cb0H7sH",
        "MS214K_VJMBZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Pyspark"
      ],
      "metadata": {
        "id": "U4hnCmqC-iDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XCi7Tvm2-lzK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YU43Vwjx-mm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "fb025091-1ba8-4db3-c0ba-e794d2982cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,832 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,532 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,151 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,475 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
            "Fetched 28.5 MB in 8s (3,640 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e91cdbaaea0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://bf5e86746b33:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q-q5BnF2-h8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kuDSzoHO-hz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pizdK6CC-hj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1JkZlxFW-bid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2G_WhYl9012"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NckTQeQH-XSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Single Column Spark Dataframe using List"
      ],
      "metadata": {
        "id": "mJZgO1Ew-Xrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bB3phNbv-o1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjhM_mNR-pUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"SingleColumnDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample list of integers\n",
        "data = [1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "\n",
        "\n",
        "# Convert the list to a DataFrame with a single column named 'Numbers'\n",
        "\n",
        "demodf = spark.createDataFrame(data,IntegerType())\n",
        "\n",
        "\n",
        "# Show the DataFrame\n",
        "demodf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHwdSbnj-vIk",
        "outputId": "f1096d36-d4c9-4765-bc01-1cdaf217cee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|value|\n",
            "+-----+\n",
            "|    1|\n",
            "|    2|\n",
            "|    3|\n",
            "|    4|\n",
            "|    5|\n",
            "|    6|\n",
            "|    7|\n",
            "|    8|\n",
            "|    9|\n",
            "|   10|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data,'int')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuK31oiZAgeU",
        "outputId": "c13fe2e0-10b4-43f8-c8fd-f654d8396014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|value|\n",
            "+-----+\n",
            "|    1|\n",
            "|    2|\n",
            "|    3|\n",
            "|    4|\n",
            "|    5|\n",
            "|    6|\n",
            "|    7|\n",
            "|    8|\n",
            "|    9|\n",
            "|   10|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"SingleColumnDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample list of strings\n",
        "data = [\"apple\",\"banana\",\"carrot\",\"cow\"]\n",
        "\n",
        "# Convert the list to a DataFrame with a single column named 'Fruits'\n",
        "df = spark.createDataFrame(data,StringType() ).toDF(\"fruits\")\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4mPhkXz-zjA",
        "outputId": "400f0226-dc46-4062-d4da-92b137796856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|fruits|\n",
            "+------+\n",
            "| apple|\n",
            "|banana|\n",
            "|carrot|\n",
            "|   cow|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df =spark.createDataFrame(data,'string')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAa55s9pBAV4",
        "outputId": "4bb0eaf2-bc70-4408-db85-311bd5019e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "| apple|\n",
            "|banana|\n",
            "|carrot|\n",
            "|   cow|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dKrjtRhPA_38"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1Q3FQ3Qn8xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"SingleColumnDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample list of tuples (even though it's a single element tuple)\n",
        "data = [(10,), (20,), (30,), (40,), (50,)]\n",
        "\n",
        "# Convert the list to a DataFrame with a single column named 'Values'\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_3TMw3o-2Mb",
        "outputId": "69057555-2def-4734-92cf-26e4b915e308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| _1|\n",
            "+---+\n",
            "| 10|\n",
            "| 20|\n",
            "| 30|\n",
            "| 40|\n",
            "| 50|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"SingleColumnDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample list of mixed data types\n",
        "data = [1, \"two\", 3.0, True, None]\n",
        "\n",
        "# Convert the list to a DataFrame with a single column named 'MixedValues'\n",
        "df = spark.createDataFrame([(i,) for i in data]).toDF(\"MixedValues\")\n",
        "#df = spark.createDataFrame(data,'string')\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saFNFzyZ-5wb",
        "outputId": "f0379c50-75cd-4327-ebd2-d5d6704aa56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|MixedValues|\n",
            "+-----------+\n",
            "|          1|\n",
            "|        two|\n",
            "|        3.0|\n",
            "|       true|\n",
            "|       NULL|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, `IntegerType` and `StringType` are data types used to define the schema of DataFrames, which represent structured data. Understanding these types is crucial for working effectively with PySpark DataFrames, as they help ensure that data is interpreted correctly during processing.\n",
        "\n",
        "### 1. **`IntegerType`**\n",
        "\n",
        "#### Explanation:\n",
        "- **`IntegerType`** is used to represent integer values in a DataFrame.\n",
        "- It maps to the integer data type, which means it can store whole numbers (both positive and negative) within a specific range (typically -2^31 to 2^31-1).\n",
        "\n",
        "#### Significance:\n",
        "- **Precision**: Ensures that the data is stored and processed as integers, which is important for operations that require numeric precision, like mathematical calculations or aggregations.\n",
        "- **Optimization**: DataFrames that use `IntegerType` can benefit from optimized performance, as integers are simpler to store and manipulate compared to floating-point numbers or strings.\n",
        "\n",
        "#### Uses:\n",
        "- **Numeric Columns**: Use `IntegerType` for columns that represent whole numbers, such as age, count, or any other numeric fields without decimals.\n",
        "- **Aggregations**: When performing operations like sum, count, or average, using `IntegerType` ensures accurate results without the overhead of dealing with decimal points.\n",
        "\n",
        "#### Example:\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IntegerTypeExample\").getOrCreate()\n",
        "\n",
        "data = [(1, \"Amit\", 29), (2, \"Priya\", 31)]\n",
        "columns = [\"ID\", \"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[IntegerType(), StringType(), IntegerType()])\n",
        "df.show()\n",
        "```\n",
        "\n",
        "### 2. **`StringType`**\n",
        "\n",
        "#### Explanation:\n",
        "- **`StringType`** is used to represent string values (text) in a DataFrame.\n",
        "- It maps to the string data type, which means it can store any sequence of characters.\n",
        "\n",
        "#### Significance:\n",
        "- **Flexibility**: `StringType` allows storing any text data, making it versatile for handling different kinds of information such as names, descriptions, or categorical data.\n",
        "- **Human-Readable**: Text data is often used for fields that need to be human-readable, like names, addresses, or any descriptive fields.\n",
        "\n",
        "#### Uses:\n",
        "- **Text Columns**: Use `StringType` for columns that store text data, such as names, product descriptions, or any categorical fields.\n",
        "- **Joining DataFrames**: When performing joins based on string columns (like matching customer names), using `StringType` ensures that the data is compared correctly.\n",
        "- **Filtering**: `StringType` allows you to easily filter DataFrames based on text criteria (e.g., find all rows where the name is \"Priya\").\n",
        "\n",
        "#### Example:\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"StringTypeExample\").getOrCreate()\n",
        "\n",
        "data = [(1, \"Amit\", 29), (2, \"Priya\", 31)]\n",
        "columns = [\"ID\", \"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[IntegerType(), StringType(), IntegerType()])\n",
        "df.show()\n",
        "```\n",
        "\n",
        "### Summary of Significance and Uses:\n",
        "- **Data Integrity**: Using `IntegerType` and `StringType` ensures that your DataFrame's schema matches the expected data types, which helps prevent errors during data processing.\n",
        "- **Performance**: Properly typed data leads to better performance, especially in large-scale data processing, as Spark can optimize operations based on data types.\n",
        "- **Ease of Use**: By defining data types explicitly, you make your code more readable and maintainable, as it’s clear what kind of data each column contains.\n",
        "\n",
        "Understanding and utilizing `IntegerType` and `StringType` correctly is essential for building robust and efficient PySpark applications that handle structured data."
      ],
      "metadata": {
        "id": "CU9QAYsNABWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oFQCclSYB12E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YawcUs1HB2cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Multi Column Spark Dataframe using Python list"
      ],
      "metadata": {
        "id": "0iyog2usB5jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"MultiColumnDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample list of tuples, each tuple representing a row\n",
        "data = [\n",
        "    (1, \"Amit\", 29),\n",
        "    (2, \"Priya\", 31),\n",
        "    (3, \"Vikram\", 25),\n",
        "    (4, \"Neha\", 40)\n",
        "]\n",
        "\n",
        "# Specify the column names\n",
        "columns = [\"ID\",\"Name\",\"Age\"]\n",
        "\n",
        "# Create the DataFrame\n",
        "df = spark.createDataFrame(data,columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgDBcda0B3Oj",
        "outputId": "ac957cfb-db29-4b34-d7fd-c79dd13439a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| ID|  Name|Age|\n",
            "+---+------+---+\n",
            "|  1|  Amit| 29|\n",
            "|  2| Priya| 31|\n",
            "|  3|Vikram| 25|\n",
            "|  4|  Neha| 40|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d1U6PKPiCpZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5TEIb8zrCp54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of row on Dataframes"
      ],
      "metadata": {
        "id": "on70yaimCqOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gClzktdQCw-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark Row:\n",
        "\n",
        "#### Introduction to PySpark Row\n",
        "- **`Row` in PySpark**: The `Row` class in PySpark is a part of the `pyspark.sql` module and represents a single record or row in a DataFrame. It is analogous to a tuple in Python but provides more flexibility and better readability when dealing with structured data.\n",
        "\n",
        "- **Structured Data Representation**: In a DataFrame, each row is a `Row` object. This structure allows you to access and manipulate each field within a row by either position (like a tuple) or by name (like a dictionary).\n",
        "\n",
        "- **Importance**: Understanding how to work with `Row` objects is essential when you need to interact with individual records in a DataFrame, especially when converting RDDs to DataFrames or dealing with complex data transformations.\n",
        "\n",
        "#### Creating and Using PySpark Row\n",
        "\n",
        "1. **Basic Creation**:\n",
        "   - A `Row` can be created manually by importing the `Row` class and passing the field values.\n",
        "   - Example:\n",
        "     ```python\n",
        "     from pyspark.sql import Row\n",
        "\n",
        "     # Creating a Row object\n",
        "     row1 = Row(ID=1, Name=\"Amit\", Age=29)\n",
        "\n",
        "     # Accessing the fields\n",
        "     print(row1.ID)  # Output: 1\n",
        "     print(row1.Name)  # Output: Amit\n",
        "     print(row1.Age)  # Output: 29\n",
        "     ```\n",
        "\n",
        "2. **Accessing Row Data**:\n",
        "   - **By Name**: You can access the data in a `Row` by the field name, which makes the code more readable and easier to understand.\n",
        "   - **By Position**: You can also access the fields by their position, similar to how you would access elements in a tuple.\n",
        "\n",
        "   - Example:\n",
        "     ```python\n",
        "     print(row1[0])  # Output: 1\n",
        "     print(row1['Name'])  # Output: Amit\n",
        "     ```\n",
        "\n",
        "3. **Using Row in DataFrame**:\n",
        "   - Rows are often used in DataFrames, where each record in the DataFrame is a `Row` object. This is particularly useful when converting RDDs to DataFrames.\n",
        "   \n",
        "   - Example:\n",
        "     ```python\n",
        "     from pyspark.sql import SparkSession\n",
        "     from pyspark.sql import Row\n",
        "\n",
        "     # Initialize SparkSession\n",
        "     spark = SparkSession.builder.appName(\"RowExample\").getOrCreate()\n",
        "\n",
        "     # Sample data in the form of Rows\n",
        "     data = [Row(ID=1, Name=\"Amit\", Age=29),\n",
        "             Row(ID=2, Name=\"Priya\", Age=31),\n",
        "             Row(ID=3, Name=\"Vikram\", Age=25)]\n",
        "\n",
        "     # Creating a DataFrame from Rows\n",
        "     df = spark.createDataFrame(data)\n",
        "\n",
        "     # Show the DataFrame\n",
        "     df.show()\n",
        "     ```\n",
        "\n",
        "   **Output**:\n",
        "   ```\n",
        "   +---+-------+---+\n",
        "   | ID|   Name|Age|\n",
        "   +---+-------+---+\n",
        "   |  1|   Amit| 29|\n",
        "   |  2|  Priya| 31|\n",
        "   |  3|Vikram| 25|\n",
        "   +---+-------+---+\n",
        "   ```\n",
        "\n",
        "4. **Dynamic Fields**:\n",
        "   - The `Row` class allows you to create Rows with dynamic fields, making it flexible for use cases where the structure of the data isn't known upfront.\n",
        "   - Example:\n",
        "     ```python\n",
        "     person = Row(\"Name\", \"Age\")\n",
        "     row2 = person(\"Neha\", 40)\n",
        "\n",
        "     print(row2.Name)  # Output: Neha\n",
        "     print(row2.Age)  # Output: 40\n",
        "     ```\n",
        "\n",
        "5. **Converting RDD to DataFrame using Row**:\n",
        "   - When working with RDDs (Resilient Distributed Datasets), you can convert them to DataFrames by mapping each RDD element to a `Row`.\n",
        "   - Example:\n",
        "     ```python\n",
        "     # Sample RDD data\n",
        "     rdd = spark.sparkContext.parallelize([\n",
        "         (1, \"Amit\", 29),\n",
        "         (2, \"Priya\", 31),\n",
        "         (3, \"Vikram\", 25)\n",
        "     ])\n",
        "\n",
        "     # Convert RDD to DataFrame using Row\n",
        "     df = rdd.map(lambda x: Row(ID=x[0], Name=x[1], Age=x[2])).toDF()\n",
        "\n",
        "     # Show the DataFrame\n",
        "     df.show()\n",
        "     ```\n",
        "\n",
        "   **Output**:\n",
        "   ```\n",
        "   +---+-------+---+\n",
        "   | ID|   Name|Age|\n",
        "   +---+-------+---+\n",
        "   |  1|   Amit| 29|\n",
        "   |  2|  Priya| 31|\n",
        "   |  3|Vikram| 25|\n",
        "   +---+-------+---+\n",
        "   ```\n",
        "\n",
        "6. **Working with Nested Structures**:\n",
        "   - `Row` can also handle nested structures, making it possible to represent complex data hierarchies.\n",
        "   - Example:\n",
        "     ```python\n",
        "     address = Row(City=\"Mumbai\", Zip=\"400001\")\n",
        "     person = Row(ID=1, Name=\"Amit\", Age=29, Address=address)\n",
        "\n",
        "     print(person.Address.City)  # Output: Mumbai\n",
        "     print(person.Address.Zip)  # Output: 400001\n",
        "     ```\n",
        "\n",
        "#### Summary of Key Points:\n",
        "- **Flexibility**: PySpark’s `Row` is highly flexible, allowing both positional and named access to data. This is particularly useful when converting RDDs to DataFrames or working with complex data structures.\n",
        "  \n",
        "- **Integration with DataFrames**: `Row` is integral to the functioning of DataFrames in PySpark, where each record in a DataFrame is represented as a `Row`.\n",
        "\n",
        "- **Dynamic and Nested Data**: Rows can be dynamically created with variable fields and can also handle nested data structures, making them suitable for complex data representations.\n",
        "\n",
        "- **Conversion of RDDs to DataFrames**: The `Row` object plays a crucial role in converting RDDs to DataFrames, providing a bridge between the unstructured world of RDDs and the structured world of DataFrames.\n",
        "\n",
        "#### Practical Examples Summary:\n",
        "1. **Creating and Accessing Rows**:\n",
        "   - Created a `Row` and accessed its fields by both name and position.\n",
        "   \n",
        "2. **Using Rows in DataFrames**:\n",
        "   - Demonstrated how to create a DataFrame from a list of `Row` objects and access the data.\n",
        "\n",
        "3. **Dynamic Row Creation**:\n",
        "   - Showed how to create `Row` objects with dynamic fields.\n",
        "\n",
        "4. **Converting RDDs to DataFrames**:\n",
        "   - Converted an RDD to a DataFrame using `Row`, illustrating the importance of `Row` in PySpark workflows.\n",
        "\n",
        "5. **Nested Rows**:\n",
        "   - Demonstrated handling nested data structures within a `Row`, emphasizing the flexibility of the `Row` class in managing complex data.\n",
        "\n",
        "Understanding and effectively using `Row` in PySpark is crucial for anyone working with PySpark, as it enables you to handle structured data more effectively and leverage the full power of DataFrames."
      ],
      "metadata": {
        "id": "Ep7XJvc4CxQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "row1=Row(Id=1,Name=\"Amit\",Age=29)\n",
        "print(row1.Id)\n",
        "print(row1.Name)\n",
        "print(row1.Age)\n",
        "print(row1['Name'])\n",
        "print(row1[0])\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"RowExample\").getOrCreate()\n",
        "\n",
        "# Sample data in the form of Rows\n",
        "data = [Row(ID=1, Name=\"Amit\", Age=29),\n",
        "        Row(ID=2, Name=\"Priya\", Age=31),\n",
        "        Row(ID=3, Name=\"Vikram\", Age=25)]\n",
        "\n",
        "# Creating a DataFrame from Rows\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "id": "38PKpR0QFSWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8240609-6137-4c56-fbae-6c65d8064f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Amit\n",
            "29\n",
            "Amit\n",
            "1\n",
            "+---+------+---+\n",
            "| ID|  Name|Age|\n",
            "+---+------+---+\n",
            "|  1|  Amit| 29|\n",
            "|  2| Priya| 31|\n",
            "|  3|Vikram| 25|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert RDD to Dataframe\n",
        "\n",
        "rdd =spark.sparkContext.parallelize([\n",
        "    (1,\"Amit\",29),\n",
        "    (2,\"Priya\",31),\n",
        "    (3,\"Vikram\",25)\n",
        "])\n",
        "rdd.collect()\n",
        "\n",
        "rdd_rows = rdd.map(lambda x: Row(ID=x[0],Name=x[1],Age=x[2]))\n",
        "df=spark.createDataFrame(rdd_rows)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mac1oag6a72",
        "outputId": "342352ee-eb0a-413a-9e1b-33875872a171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| ID|  Name|Age|\n",
            "+---+------+---+\n",
            "|  1|  Amit| 29|\n",
            "|  2| Priya| 31|\n",
            "|  3|Vikram| 25|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b8KVt_U_GoTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F46Y0-SK4-Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert List of Lists into Spark Dataframe using Row"
      ],
      "metadata": {
        "id": "TlgKZf5RGorM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "users_list = [[1, 'Scott'], [2, 'Donald'], [3, 'Mickey'], [4, 'Elvis']]\n",
        "df=spark.createDataFrame(users_list,'user_id int, user_first_name string')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6f1gOmEHC1b",
        "outputId": "c8a90021-1ec3-4a06-80f8-aef307844dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|user_id|user_first_name|\n",
            "+-------+---------------+\n",
            "|      1|          Scott|\n",
            "|      2|         Donald|\n",
            "|      3|         Mickey|\n",
            "|      4|          Elvis|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ucoP4wav-YwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"ListToDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample List of Lists\n",
        "data = [\n",
        "    [1, \"Amit\", 29],\n",
        "    [2, \"Priya\", 31],\n",
        "    [3, \"Vikram\", 25],\n",
        "    [4, \"Neha\", 40]\n",
        "]\n",
        "\n",
        "# Specify column names\n",
        "columns = [\"ID\",\"Name\",\"AGE\"]\n",
        "\n",
        "# Convert List of Lists to a List of Row objects\n",
        "row_data = [Row(ID=row[0], Name=row[1],Age=row[2]) for row in data]\n",
        "\n",
        "# Create a DataFrame from the List of Row objects\n",
        "df = spark.createDataFrame(row_data)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p77I9V1Gvzv",
        "outputId": "7f176ccc-5fe2-4d20-b794-d54b53cee01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| ID|  Name|Age|\n",
            "+---+------+---+\n",
            "|  1|  Amit| 29|\n",
            "|  2| Priya| 31|\n",
            "|  3|Vikram| 25|\n",
            "|  4|  Neha| 40|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NGjqI03fHwX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert List of tuples into Spark Dataframe using Row"
      ],
      "metadata": {
        "id": "UJdcWYjEHw2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"TupleToDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample List of Tuples\n",
        "data = [\n",
        "    (1, \"Amit\", 29),\n",
        "    (2, \"Priya\", 31),\n",
        "    (3, \"Vikram\", 25),\n",
        "    (4, \"Neha\", 40)\n",
        "]\n",
        "\n",
        "# Specify column names\n",
        "columns = [\"ID\",\"Name\",\"age\"]\n",
        "\n",
        "# Convert List of Tuples to a List of Row objects\n",
        "row_data = [Row(ID=row[0],Name=row[1],age=row[2])for row in data]\n",
        "\n",
        "# Create a DataFrame from the List of Row objects\n",
        "df = spark.createDataFrame(row_data)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puMHGvuqHxVs",
        "outputId": "75cca779-df9f-479b-be58-b618a82df2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| ID|  Name|age|\n",
            "+---+------+---+\n",
            "|  1|  Amit| 29|\n",
            "|  2| Priya| 31|\n",
            "|  3|Vikram| 25|\n",
            "|  4|  Neha| 40|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4x3haKEGH7Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert List of dict into Spark Dataframe using Row"
      ],
      "metadata": {
        "id": "8QDg5Cb0H7sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DictToDataFrame\").getOrCreate()\n",
        "\n",
        "# Sample List of Dictionaries\n",
        "data = [\n",
        "    {\"ID\": 1, \"Name\": \"Amit\", \"Age\": 29},\n",
        "    {\"ID\": 2, \"Name\": \"Priya\", \"Age\": 31},\n",
        "    {\"ID\": 3, \"Name\": \"Vikram\", \"Age\": 25},\n",
        "    {\"ID\": 4, \"Name\": \"Neha\", \"Age\": 40}\n",
        "]\n",
        "\n",
        "# Convert List of Dictionaries to a List of Row objects\n",
        "row_data = [Row(**row)for row in data]\n",
        "\n",
        "# Create a DataFrame from the List of Row objects\n",
        "df = spark.createDataFrame(row_data)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLQ2eDVFID6c",
        "outputId": "b20ef76f-c7f6-4b35-f9b7-8ac64bb956ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| ID|  Name|Age|\n",
            "+---+------+---+\n",
            "|  1|  Amit| 29|\n",
            "|  2| Priya| 31|\n",
            "|  3|Vikram| 25|\n",
            "|  4|  Neha| 40|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUB0P3DcKa0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Row Mapping:\n",
        "\n",
        "The list comprehension [Row(**row) for row in data] uses **row to unpack each dictionary into keyword arguments for Row, creating a Row object for each dictionary."
      ],
      "metadata": {
        "id": "keCHjsJrIT2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Data Types in Spark"
      ],
      "metadata": {
        "id": "MS214K_VJMBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Basic Data Types in Spark: Comprehensive Notes**\n",
        "\n",
        "Apache Spark is a powerful distributed computing framework designed for processing large-scale data. Understanding the basic data types in Spark is crucial for effectively managing and manipulating data in Spark DataFrames and RDDs (Resilient Distributed Datasets). These data types form the foundation for working with structured data in Spark SQL and DataFrames.\n",
        "\n",
        "#### **1. Overview of Spark Data Types**\n",
        "- **Categories**: Spark data types can be broadly categorized into primitive types (e.g., integers, floats, strings), complex types (e.g., arrays, maps), and structured types (e.g., structs).\n",
        "- **Usage**: These data types are used to define the schema of DataFrames, ensuring that data is stored, processed, and manipulated correctly.\n",
        "- **Interoperability**: Spark’s data types closely resemble SQL data types, making it easier to integrate with relational databases and work with structured data.\n",
        "\n",
        "#### **2. Primitive Data Types**\n",
        "Primitive data types are the basic building blocks in Spark and represent single values like integers, floating-point numbers, and strings.\n",
        "\n",
        "##### **a. IntegerType**\n",
        "- **Description**: Represents 32-bit signed integers.\n",
        "- **Range**: From -2,147,483,648 to 2,147,483,647.\n",
        "- **Usage**: Used for columns that require whole numbers without decimal points.\n",
        "- **Example**: Age, number of items, etc.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import IntegerType\n",
        "\n",
        "  # Defining a column with IntegerType\n",
        "  age_column = IntegerType()\n",
        "  ```\n",
        "\n",
        "##### **b. LongType**\n",
        "- **Description**: Represents 64-bit signed integers.\n",
        "- **Range**: From -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.\n",
        "- **Usage**: Used for columns that require large whole numbers.\n",
        "- **Example**: Big data counts, large monetary values.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import LongType\n",
        "\n",
        "  # Defining a column with LongType\n",
        "  large_number_column = LongType()\n",
        "  ```\n",
        "\n",
        "##### **c. FloatType**\n",
        "- **Description**: Represents 32-bit floating-point numbers.\n",
        "- **Precision**: Approximately 7 decimal digits of precision.\n",
        "- **Usage**: Used for columns that require decimal numbers but with limited precision.\n",
        "- **Example**: Measurements, ratings, etc.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import FloatType\n",
        "\n",
        "  # Defining a column with FloatType\n",
        "  rating_column = FloatType()\n",
        "  ```\n",
        "\n",
        "##### **d. DoubleType**\n",
        "- **Description**: Represents 64-bit floating-point numbers.\n",
        "- **Precision**: Approximately 15 decimal digits of precision.\n",
        "- **Usage**: Used for columns that require high precision with decimal numbers.\n",
        "- **Example**: Scientific calculations, precise financial data.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import DoubleType\n",
        "\n",
        "  # Defining a column with DoubleType\n",
        "  precise_measurement_column = DoubleType()\n",
        "  ```\n",
        "\n",
        "##### **e. StringType**\n",
        "- **Description**: Represents string values (text).\n",
        "- **Usage**: Used for columns that require text data.\n",
        "- **Example**: Names, descriptions, categories, etc.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import StringType\n",
        "\n",
        "  # Defining a column with StringType\n",
        "  name_column = StringType()\n",
        "  ```\n",
        "\n",
        "##### **f. BooleanType**\n",
        "- **Description**: Represents boolean values (`True` or `False`).\n",
        "- **Usage**: Used for columns that require binary decisions or flags.\n",
        "- **Example**: IsActive status, boolean checks.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import BooleanType\n",
        "\n",
        "  # Defining a column with BooleanType\n",
        "  active_status_column = BooleanType()\n",
        "  ```\n",
        "\n",
        "##### **g. ByteType**\n",
        "- **Description**: Represents 8-bit signed integers.\n",
        "- **Range**: From -128 to 127.\n",
        "- **Usage**: Used for very small integer values.\n",
        "- **Example**: Small counters, binary data in integer form.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import ByteType\n",
        "\n",
        "  # Defining a column with ByteType\n",
        "  small_counter_column = ByteType()\n",
        "  ```\n",
        "\n",
        "##### **h. ShortType**\n",
        "- **Description**: Represents 16-bit signed integers.\n",
        "- **Range**: From -32,768 to 32,767.\n",
        "- **Usage**: Used for columns that require small integer values but more than `ByteType`.\n",
        "- **Example**: Smaller numerical ranges, minor levels.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import ShortType\n",
        "\n",
        "  # Defining a column with ShortType\n",
        "  minor_level_column = ShortType()\n",
        "  ```\n",
        "\n",
        "##### **i. BinaryType**\n",
        "- **Description**: Represents binary data.\n",
        "- **Usage**: Used for columns that store binary data, such as images or encrypted data.\n",
        "- **Example**: Image files, encrypted strings, serialized data.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import BinaryType\n",
        "\n",
        "  # Defining a column with BinaryType\n",
        "  image_data_column = BinaryType()\n",
        "  ```\n",
        "\n",
        "##### **j. DecimalType**\n",
        "- **Description**: Represents arbitrary-precision signed decimal numbers.\n",
        "- **Usage**: Used for columns that require precise decimal numbers with a specific scale and precision.\n",
        "- **Example**: Financial data, precise measurements.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import DecimalType\n",
        "\n",
        "  # Defining a column with DecimalType (precision = 10, scale = 2)\n",
        "  financial_data_column = DecimalType(10, 2)\n",
        "  ```\n",
        "\n",
        "#### **3. Complex Data Types**\n",
        "Complex data types in Spark are used to represent collections or structured data within a single column.\n",
        "\n",
        "##### **a. ArrayType**\n",
        "- **Description**: Represents a column of arrays (lists).\n",
        "- **Usage**: Used for columns that need to store multiple values in a single field.\n",
        "- **Example**: Tags, list of features, etc.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "  # Defining a column with ArrayType containing strings\n",
        "  tags_column = ArrayType(StringType())\n",
        "  data = [\n",
        "    (1, \"Amit\", [\"Reading\", \"Traveling\", \"Music\"]),\n",
        "    (2, \"Priya\", [\"Cooking\", \"Yoga\"]),\n",
        "    (3, \"Vikram\", [\"Gaming\", \"Hiking\", \"Photography\"]),\n",
        "    (4, \"Neha\", [\"Dancing\", \"Painting\"])\n",
        "]\n",
        "  schema = StructType([\n",
        "    StructField(\"ID\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Hobbies\", ArrayType(StringType()), True)\n",
        "])\n",
        "  ```\n",
        "\n",
        "##### **b. MapType**\n",
        "- **Description**: Represents a column of key-value pairs (maps).\n",
        "- **Usage**: Used for columns that store key-value pairs within a single field.\n",
        "- **Example**: Metadata, configuration settings.\n",
        "- **Code Example**:\n",
        "\n",
        "  ```python\n",
        "  from pyspark.sql.types import MapType, StringType, IntegerType\n",
        "\n",
        "  # Defining a column with MapType with String keys and Integer values\n",
        "  metadata_column = MapType(StringType(), IntegerType())\n",
        "  data = [\n",
        "    (1, \"Amit\", {\"Math\": 85, \"Science\": 90}),\n",
        "    (2, \"Priya\", {\"Math\": 95, \"English\": 88}),\n",
        "    (3, \"Vikram\", {\"Science\": 75, \"History\": 80}),\n",
        "    (4, \"Neha\", {\"English\": 92, \"Math\": 89})\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "##### **c. StructType**\n",
        "- **Description**: Represents a column of nested fields (like a record or struct).\n",
        "- **Usage**: Used for columns that store a complex structure with multiple fields.\n",
        "- **Example**: Address (with fields like street, city, zip).\n",
        "- **Code Example**:\n",
        "\n",
        "  ```python\n",
        "  from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "  # Defining a column with StructType (nested fields)\n",
        "  address_column = StructType([\n",
        "      StructField(\"Street\", StringType(), True),\n",
        "      StructField(\"City\", StringType(), True),\n",
        "      StructField(\"Zip\", IntegerType(), True)\n",
        "  ])\n",
        "\n",
        "  data = [\n",
        "    (1, \"Amit\", (\"123 Main St\", \"Mumbai\", \"400001\")),\n",
        "    (2, \"Priya\", (\"456 Park Ave\", \"Delhi\", \"110001\")),\n",
        "    (3, \"Vikram\", (\"789 Hill Rd\", \"Bangalore\", \"560001\")),\n",
        "    (4, \"Neha\", (\"101 Lake View\", \"Chennai\", \"600001\"))\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Address\", StructType([\n",
        "        StructField(\"Street\", StringType(), True),\n",
        "        StructField(\"City\", StringType(), True),\n",
        "        StructField(\"ZipCode\", StringType(), True)\n",
        "    ]), True)\n",
        "])\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "#### **4. Special Data Types**\n",
        "Special data types in Spark are used for specific scenarios such as dates and timestamps.\n",
        "\n",
        "\n",
        "\n",
        "##### **a. DateType**\n",
        "- **Description**: Represents a date without a time component.\n",
        "- **Usage**: Used for columns that store dates.\n",
        "- **Example**: Birthdates, event dates.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import DateType\n",
        "\n",
        "  # Defining a column with DateType\n",
        "  birthdate_column = DateType()\n",
        "\n",
        "  data = [\n",
        "    (1, \"Amit\", \"2023-01-01\"),\n",
        "    (2, \"Priya\", \"2023-02-15\"),\n",
        "    (3, \"Vikram\", \"2023-03-20\"),\n",
        "    (4, \"Neha\", \"2023-04-25\")\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Birthdate\", DateType(), True)\n",
        "])\n",
        "\n",
        "  ```\n",
        "\n",
        "\n",
        "##### **b. TimestampType**\n",
        "- **Description**: Represents a timestamp (date and time).\n",
        "- **Usage**: Used for columns that store date and time.\n",
        "- **Example**: Event logs, transaction times.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import TimestampType\n",
        "\n",
        "  # Defining a column with TimestampType\n",
        "  event_timestamp_column = TimestampType()\n",
        "\n",
        "  data = [\n",
        "    (1, \"Amit\", \"2023-01-01 10:00:00\"),\n",
        "    (2, \"Priya\", \"2023-02-15 14:30:00\"),\n",
        "    (3, \"Vikram\", \"2023-03-20 09:15:00\"),\n",
        "    (4, \"Neha\", \"2023-04-25 18:45:00\")\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"EventTime\", TimestampType(), True)\n",
        "])\n",
        " ```\n",
        "\n",
        "\n",
        "#### **5. NullType**\n",
        "- **Description**: Represents a null value or unknown type.\n",
        "- **Usage**: Generally used for internal purposes or as a placeholder in some cases.\n",
        "- **Example**: Undefined fields, default values during schema evolution.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from pyspark.sql.types import NullType\n",
        "\n",
        "  # Defining a column with NullType (rarely used directly)\n",
        "  undefined_column = NullType()\n",
        "  ```\n",
        "\n",
        "#### **6. Type Conversion in Spark**\n",
        "- **Automatic Type Conversion**: Spark can automatically infer types when reading data from sources like CSV, Parquet, or JSON, and can cast data types during operations where necessary.\n",
        "- **Explicit Type Casting**: You can cast columns from one type to another using the `cast()` function.\n",
        "  - **Example**:\n",
        "    ```python\n",
        "    df = df.withColumn(\"Age\", df[\"Age\"].cast(IntegerType()))\n",
        "    ```\n",
        "\n",
        "#### **7. Summary of Spark Data Types**\n",
        "- **Primitive Types**: Handle single values (e.g., IntegerType, StringType).\n",
        "- **Complex Types**: Handle collections or nested data (\n",
        "\n",
        "e.g., ArrayType, StructType).\n",
        "- **Special Types**: Handle dates, timestamps, and nulls (e.g., DateType, TimestampType, NullType).\n",
        "- **Usage**: Understanding these data types helps in defining schemas, performing data transformations, and optimizing data processing in Spark.\n",
        "\n",
        "#### **8. Importance of Choosing the Right Data Type**\n",
        "- **Performance**: Proper data types ensure efficient memory usage and faster computations.\n",
        "- **Accuracy**: Choosing the right type prevents data loss or inaccuracies (e.g., using `DecimalType` for financial calculations).\n",
        "- **Interoperability**: Proper types ensure seamless integration with databases and external systems.\n",
        "\n",
        "By understanding and effectively using Spark’s data types, you can better manage your data, write more efficient queries, and develop more robust data processing pipelines. Whether you are defining schemas for DataFrames or working with RDDs, knowing these data types is fundamental to leveraging the full power of Spark."
      ],
      "metadata": {
        "id": "1ozWIkPuJcoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType\n",
        "\n",
        "# Defining a column with ArrayType containing strings\n",
        "tags_column = ArrayType(StringType())\n",
        "data = [\n",
        "  (1, \"Amit\", [\"Reading\", \"Traveling\", \"Music\"]),\n",
        "  (2, \"Priya\", [\"Cooking\", \"Yoga\"]),\n",
        "  (3, \"Vikram\", [\"Gaming\", \"Hiking\", \"Photography\"]),\n",
        "  (4, \"Neha\", [\"Dancing\", \"Painting\"])\n",
        "]\n",
        "schema = StructType([\n",
        "  StructField(\"ID\", IntegerType(), True),\n",
        "  StructField(\"Name\", StringType(), True),\n",
        "  StructField(\"Hobbies\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType\n",
        "\n",
        "# Defining a column with MapType with String keys and Integer values\n",
        "metadata_column = MapType(StringType(), IntegerType())\n",
        "data = [\n",
        "  (1, \"Amit\", {\"Math\": 85, \"Science\": 90}),\n",
        "  (2, \"Priya\", {\"Math\": 95, \"English\": 88}),\n",
        "  (3, \"Vikram\", {\"Science\": 75, \"History\": 80}),\n",
        "  (4, \"Neha\", {\"English\": 92, \"Math\": 89})\n",
        "]\n",
        "schema = StructType([\n",
        "  StructField(\"ID\", IntegerType(), True),\n",
        "  StructField(\"Name\", StringType(), True),\n",
        "  StructField(\"Scores\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "MMlvB8KSCkfx",
        "outputId": "e7fb4c87-a7a9-447b-bddb-9ec88fcf5926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------------------+\n",
            "| ID|  Name|             Hobbies|\n",
            "+---+------+--------------------+\n",
            "|  1|  Amit|[Reading, Traveli...|\n",
            "|  2| Priya|     [Cooking, Yoga]|\n",
            "|  3|Vikram|[Gaming, Hiking, ...|\n",
            "|  4|  Neha| [Dancing, Painting]|\n",
            "+---+------+--------------------+\n",
            "\n",
            "+---+------+--------------------+\n",
            "| ID|  Name|              Scores|\n",
            "+---+------+--------------------+\n",
            "|  1|  Amit|{Science -> 90, M...|\n",
            "|  2| Priya|{Math -> 95, Engl...|\n",
            "|  3|Vikram|{Science -> 75, H...|\n",
            "|  4|  Neha|{Math -> 89, Engl...|\n",
            "+---+------+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[ID: int, Name: string, Scores: map<string,int>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "KSk7wvSQXaHJ",
        "outputId": "b48adb03-8610-4cb4-d08a-6055dea3c2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[ID: int, Name: string, Scores: map<string,int>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "address = Row(City=\"Mumbai\", Zip=\"400001\")\n",
        "person = Row(ID=1, Name=\"Amit\", Age=29, Address=address)\n",
        "\n",
        "print(person.Address.City)  # Output: Mumbai\n",
        "print(person.Address.Zip)  # Output: 400001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfJCfsI7vsyq",
        "outputId": "e6974d6f-d7ab-4ee6-bd7a-bbaaeb22be2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mumbai\n",
            "400001\n"
          ]
        }
      ]
    }
  ]
}