{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9yXqV3LigUA",
        "AJb6LmAjSgVv",
        "2INI_WqHKXMe",
        "xQiZxXUqNeSo",
        "pieVJ295cmYw",
        "bHrehZfsjLjt",
        "PA27NFNWqTto"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5oXV0Adr1DKv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "892478da-4c41-4d9e-f1a2-0b77a9adf7fe"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r                                                                               \rHit:6 https://cli.github.com/packages stable InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,596 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,835 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,539 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,290 kB]\n",
            "Fetched 37.5 MB in 7s (5,398 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x791ec0c074d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://da93bae3d55a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE A RDD"
      ],
      "metadata": {
        "id": "AJb6LmAjSgVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, there are several ways to create an RDD (Resilient Distributed Dataset), each suited to different scenarios\n",
        "\n",
        "1. parallelize Method\n",
        "\n",
        "Description: The parallelize method is used to create an RDD from an existing Python collection (like a list or set). This is the simplest way to create an RDD from in-memory data.\n",
        "\n",
        "Use Case: When you have a small dataset in memory that you want to distribute across a cluster\n",
        "\n",
        " Parallelize the data\n",
        "\n",
        "rdd = sparkContext.parallelize(data)\n",
        "\n",
        "2. textFile Method\n",
        "\n",
        "Description: The textFile method is used to create an RDD from a text file stored in a distributed file system like HDFS, S3, or even local file systems.\n",
        "Use Case: When you want to create an RDD from data stored in a text file, with each line of the file becoming an element in the RDD.\n",
        "\n",
        "rdd1=sparkContext.textFile(\"/content/random_words_large.txt\")\n",
        "\n",
        "3. range Method\n",
        "\n",
        "Description: This method creates an RDD from a specified range of integers.\n",
        "Use Case: When you need to generate a sequence of numbers as an RDD.\n",
        "\n",
        "rdd = sparkContext.range(start=0, end=100, step=1, numSlices=5)\n"
      ],
      "metadata": {
        "id": "7iV36fo5W0IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sparkContext = spark.sparkContext\n",
        "\n",
        "# Your data\n",
        "data = [1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "# Parallelize the data\n",
        "\n",
        "demo = sparkContext.parallelize(data) #\n",
        "demo.collect()\n"
      ],
      "metadata": {
        "id": "2R5EMnmdSlQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bad16f3-6388-4ef1-bfc4-4db751b665ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RDD from a textFile\n",
        "rdd1=sparkContext.textFile(\"/content/pro.txt\")\n",
        "rdd1.take(n)\n",
        "\n"
      ],
      "metadata": {
        "id": "rmpQWuCZHU_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bd05e5-fee6-4b5b-d466-f3ab3c549032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one floder -> ',\n",
              " '',\n",
              " 'raw/year/month/file.txt',\n",
              " '',\n",
              " 'raw/sub',\n",
              " '',\n",
              " '',\n",
              " 'Create an interactive presentation with the LEVELUP design system using these specifications:',\n",
              " '',\n",
              " '**BRANDING & LOGO:**',\n",
              " '- LEVELUP logo in top-left corner with pulsing animation',\n",
              " '- Font: Bold, 2em size, color: #00d4ff',\n",
              " '- Animation: Pulse effect with glow (scale 1 to 1.1, text-shadow glow)',\n",
              " '',\n",
              " '**COLOR PALETTE:**',\n",
              " 'Primary Colors:',\n",
              " '- Background: Linear gradient (135deg, #667eea 0%, #764ba2 100%)',\n",
              " '- Accent Blue: #00d4ff (cyan/turquoise)',\n",
              " '- Accent Pink: #ff6b9d (coral pink)',\n",
              " '- Accent Yellow: #ffff00 (bright yellow)']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RDD from Range\n",
        "\n",
        "rdd2 = sparkContext.range(start=5, end=70, step=2)\n",
        "rdd2.take(20)\n",
        "\n"
      ],
      "metadata": {
        "id": "kFAscyKDa2ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0250544f-4734-4d9d-b5b8-0284792abff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map and FlatMap Transformation"
      ],
      "metadata": {
        "id": "2INI_WqHKXMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What is map operation ?\n",
        "\n",
        "The map operation in Spark is like a factory that takes something in and produces something else. Imagine you have a bunch of raw materials (data), and you want to process each one in a certain way to create a new product.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you have a list of numbers, and you want to create a new list where each number is doubled.\n",
        "\n",
        "Original List:\n",
        "\n",
        "[1, 2, 3, 4, 5]\n",
        "\n",
        "What you want to do:\n",
        "\n",
        "Double each number, so 1 becomes 2, 2 becomes 4, and so on.\n",
        "\n",
        "How map works:\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "       \n",
        "numrdd = sparkContext.parallelize(numbers)\n",
        "doubled_numbers = numrdd.map(lambda x: x * 2)\n",
        "doubled_numbers.take(2)\n"
      ],
      "metadata": {
        "id": "YUnj3bOvKlSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers =[1,2,3,4,5]\n",
        "numberrdd= sparkContext.parallelize(numbers)\n",
        "maprdd = numberrdd.map(lambda x: x*5)\n",
        "maprdd.take(2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Jhz27rvX8q",
        "outputId": "d3d67268-0b62-4880-b23f-ec430e80d26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [1, 2, 3, 4, 5]\n",
        "numrdd = sparkContext.parallelize(numbers)\n",
        "doubled_numbers = numrdd.map(lambda x: x * 2)\n",
        "doubled_numbers.take(2)"
      ],
      "metadata": {
        "id": "UukYzyO-Kcz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2: Adding a Suffix to Words\n",
        "\n",
        "Scenario:\n",
        "Imagine you have a list of words, and you want to add the suffix “-ly” to each word to create an adverb.\n",
        "\n",
        "Original List:\n",
        "\n",
        "[\"quick\", \"bright\", \"silent\"]\n",
        "\n",
        "What you want to do:\n",
        "\n",
        "Add “-ly” to the end of each word.\n",
        "\n",
        "How map works:\n",
        "\n",
        "You use map to append the suffix “-ly” to each word."
      ],
      "metadata": {
        "id": "DP6PQj7vLe8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - 2\n",
        "words = [\"quick\", \"bright\", \"silent\"]\n",
        "wordrdd = sparkContext.parallelize(words)\n",
        "rdd2=wordrdd.map(lambda x: x+\"ly\")\n",
        "rdd2.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Resulting List\n",
        "# [\"quickly\", \"brightly\", \"silently\"]\n"
      ],
      "metadata": {
        "id": "QYL6xzTiLc0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532ae329-7b2e-4563-b754-69446d2c9f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['quickly', 'brightly', 'silently']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlatMap Operation\n",
        "\n"
      ],
      "metadata": {
        "id": "xQiZxXUqNeSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is flatMap?\n",
        "\n",
        "The flatMap operation is a bit like map, but with a twist. Instead of just transforming each item in the list into a new item, it can turn each item into zero, one, or many items, and then flatten all those items into a single list.\n",
        "Example:\n",
        "\n",
        "Imagine you have a list of sentences, and you want to create a list of all the words in those sentences.\n",
        "\n",
        "Original List:\n",
        "\n",
        "[\"Hello world\", \"Spark is great\", \"Map and flatMap\"]\n",
        "\n",
        "What you want to do:\n",
        "\n",
        "Break each sentence into words and make a single list of all the words.\n",
        "\n",
        "How flatMap works:\n",
        "\n",
        "You tell flatMap to split each sentence into words, and then it combines all the words into a single list."
      ],
      "metadata": {
        "id": "sPAfCmShi-Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Hello world\", \"Spark is great\", \"Map and flatMap\"]\n",
        "\n",
        "rrd4 = sparkContext.parallelize(sentences)\n",
        "characters = rrd4.flatMap(lambda x:x.split(\" \"))\n",
        "characters.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIJuoYWwNo-2",
        "outputId": "f0fc5d5b-918c-420e-fc9f-d258eceb36f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'world', 'Spark', 'is', 'great', 'Map', 'and', 'flatMap']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2: Extracting Individual Characters from Words\n",
        "\n",
        "Scenario:\n",
        "You have a list of words, and you want to break each word into its individual characters. The goal is to create a list that contains all the characters from all the words.\n",
        "\n",
        "Original List of Words:\n",
        "\n",
        "[\"apple\", \"banana\", \"grape\"]\n",
        "\n",
        "How flatMap works:\n",
        "\n",
        "You apply the flatMap function to break each word into its individual characters, and then flatten all the characters into a single list."
      ],
      "metadata": {
        "id": "M1Y8tRlLOTqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"apple\", \"banana\", \"grape\"]\n",
        "rddwords = sparkContext.parallelize(words)\n",
        "characters = rddwords.flatMap(lambda x:list(x))\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "#characters.take(2)\n",
        "characters.collect()"
      ],
      "metadata": {
        "id": "msiW_vOqOZJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fa36fd-4469-487b-ce35-0d75282c049d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'p',\n",
              " 'p',\n",
              " 'l',\n",
              " 'e',\n",
              " 'b',\n",
              " 'a',\n",
              " 'n',\n",
              " 'a',\n",
              " 'n',\n",
              " 'a',\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 'p',\n",
              " 'e']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sparkContext.range(start=0, end=100, step=2, numSlices=5)\n",
        "rdd.collect()"
      ],
      "metadata": {
        "id": "LBJzPaHPZHnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReduceByKey"
      ],
      "metadata": {
        "id": "pieVJ295cmYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LMbBeFgqcjgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reduceByKey transformation in PySpark is used to aggregate values by key in a key-value pair RDD (also known as a Pair RDD). It combines values with the same key using a specified associative and commutative function (such as addition, multiplication, etc.).\n",
        "\n",
        "\n",
        "\n",
        "1) List of sales records (product, amount)\n",
        "\n",
        "sales = [(\"apple\", 100), (\"banana\", 200), (\"apple\", 150), (\"orange\", 300), (\"banana\", 50)]\n",
        "\n",
        "2) Parallelize the data to create an RDD\n",
        "\n",
        "rdd = sc.parallelize(sales)\n",
        "\n",
        "3) Use reduceByKey to sum the sales amounts by product\n",
        "\n",
        "total_sales_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "\n",
        "\n",
        "Think of reduceByKey as a process where you group items by category and then apply some operation to combine or summarize the values in each group. For instance, if you have bags of different types of fruits, reduceByKey would allow you to count how many apples, bananas, and oranges you have in total by going through each bag and adding the counts for each fruit."
      ],
      "metadata": {
        "id": "o5FXAQ7NdrBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example-01\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"ReduceByKeyExample2\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# List of sales records (product, amount)\n",
        "sales = [(\"apple\", 100), (\"banana\", 200), (\"apple\", 150), (\"orange\", 300), (\"banana\", 50)]\n",
        "\n",
        "# Parallelize the data to create an RDD\n",
        "rdd = sc.parallelize(sales)\n",
        "\n",
        "# Use reduceByKey to sum the sales amounts by product\n",
        "total_sales_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the results\n",
        "total_sales = total_sales_rdd.collect()\n",
        "\n",
        "print(total_sales)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60dh6Ug_fkQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a36200c-58fe-45a7-bae2-8ce75da55007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apple', 250), ('banana', 250), ('orange', 300)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example-02\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"ReduceByKeyWordCountExample\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# List of words\n",
        "words = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"]\n",
        "\n",
        "# Parallelize the data to create an RDD\n",
        "rdd = sc.parallelize(words)\n",
        "\n",
        "# Map each word to a key-value pair (word, 1)\n",
        "pairs_rdd = rdd.map(lambda word: (word, 1))\n",
        "\n",
        "# Use reduceByKey to count the occurrences of each word\n",
        "word_counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the results\n",
        "word_counts = word_counts_rdd.collect()\n",
        "\n",
        "print(word_counts)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "0aN1SxYIhW1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZK0HEMNjaqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SortByKey"
      ],
      "metadata": {
        "id": "bHrehZfsjLjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sortByKey Transformation in PySpark\n",
        "\n",
        "Description:\n",
        "\n",
        "The sortByKey transformation in PySpark is used to sort an RDD of key-value pairs by the key. It returns a new RDD with the elements sorted according to the keys in either ascending or descending order. This transformation is particularly useful when you want to organize data based on the keys.\n",
        "\n",
        "Key-Value Pair RDD: sortByKey only works on RDDs where each element is a key-value pair\n",
        "(i.e., a tuple (key, value)).\n",
        "\n",
        "Ascending/Descending Order: By default, sortByKey sorts the keys in ascending order. You can set it to descending order by specifying an optional parameter.\n",
        "Stable Sort: The sorting is stable, meaning that if two keys are equal, their original order is preserved."
      ],
      "metadata": {
        "id": "uu_qlmlCjevg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  example-01\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SortByKeyExample\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# List of student names and scores\n",
        "student_scores = [(\"John\", 88), (\"Alice\", 95), (\"Bob\", 78), (\"Diana\", 85)]\n",
        "\n",
        "# Parallelize the data to create an RDD\n",
        "rdd = sparkContext.parallelize(student_scores)\n",
        "sort_rdd=rdd.sortByKey(ascending=False)\n",
        "\n",
        "\n",
        "# Collect the results\n",
        "sorted_scores = sort_rdd.collect()\n",
        "\n",
        "print(sorted_scores)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7zX9n2zwn6GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9977ab-5e42-43c2-fd83-07f5719cbb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 88), ('Diana', 85), ('Bob', 78), ('Alice', 95)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example -02\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SortByKeyDescendingExample\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# List of product names and sales\n",
        "sales_data = [(\"apple\", 150), (\"banana\", 200), (\"orange\", 300), (\"grape\", 100)]\n",
        "\n",
        "# Parallelize the data to create an RDD\n",
        "rdd=sparkContext.parallelize(sales_data)  # T\n",
        "sorted_rdd = rdd.sortByKey(ascending=False) # T\n",
        "\n",
        "# Collect the results\n",
        "newemo = sorted_rdd.collect()\n",
        "\n",
        "print(newemo)\n",
        "\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "\n",
        "\n",
        "# Use sortByKey to sort the RDD by product names in descending order\n",
        "\n",
        "\n",
        "# Collect the results\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9WmTtEhoKVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6daee34e-760e-40fa-fca2-68b42b9b8d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('orange', 300), ('grape', 100), ('banana', 200), ('apple', 150)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SortBy"
      ],
      "metadata": {
        "id": "PA27NFNWqTto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sortBy transformation in PySpark is used to sort an RDD based on the values derived from each element by applying a given function. Unlike sortByKey, which only sorts based on the key in key-value pair RDDs, sortBy provides more flexibility as it allows you to specify any function that extracts a value to sort by. This function can be applied to each element of the RDD, and the RDD will be sorted based on the result of this function.\n",
        "\n",
        "Flexible Sorting: You can sort by any derived value, not just by keys.\n",
        "Ascending/Descending Order: The sorting can be done in either ascending or descending order by specifying the ascending parameter.\n",
        "Number of Partitions: You can also specify the number of partitions in the resulting RDD using the numPartitions parameter."
      ],
      "metadata": {
        "id": "5Nn_TGdyqZ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example-01\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SortByExample1\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# List of student names and scores\n",
        "student_scores = [(\"John\", 88), (\"Alice\", 95), (\"Bob\", 78), (\"Diana\", 85)]\n",
        "\n",
        "\n",
        "# Parallelize the data to create an RDD\n",
        "rdd= sparkContext.parallelize(student_scores)\n",
        "\n",
        "\n",
        "\n",
        "# Use sortBy to sort the RDD by scores (second element of the tuple)\n",
        "sorted_rdd = rdd.sortBy(lambda x: x[1],ascending=False)\n",
        "\n",
        "\n",
        "# Collect the results\n",
        "sorted_scores = sorted_rdd.collect()\n",
        "\n",
        "print(sorted_scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VM-0oOinqsVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b39de24-b2cd-4003-a603-10548a455c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 95), ('John', 88), ('Diana', 85), ('Bob', 78)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import GeneratorType\n",
        "# Example-02\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SortByExample2\").getOrCreate()\n",
        "\n",
        "# Get the Spark context from the session\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# List of words\n",
        "words = [\"banana\", \"apple\", \"grape\", \"pineapple\", \"orange\"]\n",
        "\n",
        "# Parallelize the data to create an RDD\n",
        "rdd = sparkContext.parallelize(words)\n",
        "\n",
        "# Use sortBy to sort the RDD by word length in descending order\n",
        "\n",
        "sorted_rdd = rdd.sortBy(lambda x: len(x), ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Collect the results\n",
        "sorted_words = sorted_rdd.collect()\n",
        "\n",
        "\n",
        "\n",
        "# Collect the results\n",
        "\n",
        "\n",
        "print(sorted_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nuc8PBYbq9AG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4b9606-97b9-49da-c60a-4d5d66d148bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pineapple', 'banana', 'orange', 'apple', 'grape']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to creata a RDD\n",
        "map transformation\n",
        "flat map transformation\n",
        "sortbykey\n",
        "sortby\n",
        "Differnce btw sortby and sortbyKey\n",
        "\n",
        "Action:-\n",
        "take\n",
        "collect"
      ],
      "metadata": {
        "id": "MT2imVGd8FpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "\n",
        "# Sample data: (customer_id, product_category, quantity, price_per_unit)\n",
        "sales_data = [\n",
        "    (\"C001\", \"Electronics\", 2, 599.99),\n",
        "    (\"C002\", \"Books\", 5, 15.50),\n",
        "    (\"C001\", \"Clothing\", 1, 79.99),\n",
        "    (\"C003\", \"Electronics\", 1, 1299.99),\n",
        "    (\"C002\", \"Electronics\", 3, 299.99),\n",
        "    (\"C004\", \"Books\", 2, 25.00),\n",
        "    (\"C003\", \"Clothing\", 4, 45.99),\n",
        "    (\"C001\", \"Books\", 8, 12.99),\n",
        "    (\"C004\", \"Electronics\", 1, 899.99),\n",
        "    (\"C002\", \"Clothing\", 2, 89.99)\n",
        "]\n",
        "\n",
        "# Create RDD\n"
      ],
      "metadata": {
        "id": "K3Tt4l_VM-kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark RDD Transformations Assignment\n",
        "\n",
        "## Objective\n",
        "Practice PySpark RDD transformations: `map`, `flatMap`, `sortBy`, and `sortByKey` using real-world scenarios.\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 1: E-commerce Sales Analytics\n",
        "\n",
        "### Scenario\n",
        "You work for an e-commerce company and need to analyze customer purchase data to generate insights for the marketing team.\n",
        "\n",
        "### Dataset\n",
        "```python\n",
        "# Sample data: (customer_id, product_category, quantity, price_per_unit)\n",
        "sales_data = [\n",
        "    (\"C001\", \"Electronics\", 2, 599.99),\n",
        "    (\"C002\", \"Books\", 5, 15.50),\n",
        "    (\"C001\", \"Clothing\", 1, 79.99),\n",
        "    (\"C003\", \"Electronics\", 1, 1299.99),\n",
        "    (\"C002\", \"Electronics\", 3, 299.99),\n",
        "    (\"C004\", \"Books\", 2, 25.00),\n",
        "    (\"C003\", \"Clothing\", 4, 45.99),\n",
        "    (\"C001\", \"Books\", 8, 12.99),\n",
        "    (\"C004\", \"Electronics\", 1, 899.99),\n",
        "    (\"C002\", \"Clothing\", 2, 89.99)\n",
        "]\n",
        "\n",
        "# Create RDD\n",
        "sales_rdd = sc.parallelize(sales_data)\n",
        "```\n",
        "\n",
        "### Tasks\n",
        "\n",
        "#### Task 1.1: Using `map` transformation\n",
        "Calculate the total amount spent per transaction.\n",
        "- **Input**: `(customer_id, product_category, quantity, price_per_unit)`\n",
        "- **Output**: `(customer_id, product_category, total_amount)`\n",
        "- **Formula**: `total_amount = quantity * price_per_unit`\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format: (\"C001\", \"Electronics\", 1199.98)\n",
        "```\n",
        "\n",
        "#### Task 1.2: Using `flatMap` transformation\n",
        "Extract all unique words from product categories and create a flattened list.\n",
        "- **Input**: RDD with product categories\n",
        "- **Output**: Flattened RDD of individual words\n",
        "- **Note**: Split category names by spaces if any exist, convert to lowercase\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output: [\"electronics\", \"books\", \"clothing\", ...]\n",
        "```\n",
        "\n",
        "#### Task 1.3: Using `sortBy` transformation\n",
        "Sort customers by their total spending across all transactions (descending order).\n",
        "- First calculate total spending per customer\n",
        "- Then sort by total amount in descending order\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format: [(\"C003\", total_amount), (\"C001\", total_amount), ...]\n",
        "```\n",
        "\n",
        "#### Task 1.4: Using `sortByKey` transformation\n",
        "Create category-wise sales summary and sort by category name.\n",
        "- **Input**: Calculate total sales amount per category\n",
        "- **Output**: `(category, total_sales)` sorted by category name\n",
        "- **Use**: `sortByKey()` for alphabetical sorting\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format: [(\"Books\", total_amount), (\"Clothing\", total_amount), (\"Electronics\", total_amount)]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 2: Social Media Analytics\n",
        "\n",
        "### Scenario\n",
        "You're analyzing social media posts for a digital marketing agency to understand engagement patterns and trending topics.\n",
        "\n",
        "### Dataset\n",
        "```python\n",
        "# Sample data: (user_id, post_content, likes, shares, timestamp)\n",
        "social_posts = [\n",
        "    (\"U001\", \"Amazing sunset photography tips #photography #nature\", 45, 12, \"2024-01-15\"),\n",
        "    (\"U002\", \"Best coding practices for beginners #coding #python #java\", 78, 23, \"2024-01-16\"),\n",
        "    (\"U003\", \"Delicious homemade pizza recipe #food #cooking\", 134, 45, \"2024-01-15\"),\n",
        "    (\"U001\", \"Mountain hiking adventure #nature #hiking #adventure\", 89, 18, \"2024-01-17\"),\n",
        "    (\"U004\", \"AI and machine learning trends #AI #ML #technology\", 156, 67, \"2024-01-16\"),\n",
        "    (\"U002\", \"Web development frameworks comparison #webdev #react #angular\", 92, 34, \"2024-01-18\"),\n",
        "    (\"U003\", \"Healthy breakfast ideas #food #health #nutrition\", 67, 15, \"2024-01-17\"),\n",
        "    (\"U005\", \"Travel photography in Europe #photography #travel\", 203, 89, \"2024-01-18\"),\n",
        "    (\"U004\", \"Data science tools and techniques #datascience #python\", 98, 41, \"2024-01-19\"),\n",
        "    (\"U001\", \"Sunset time-lapse creation #photography #timelapse\", 112, 28, \"2024-01-19\")\n",
        "]\n",
        "\n",
        "# Create RDD\n",
        "posts_rdd = sc.parallelize(social_posts)\n",
        "```\n",
        "\n",
        "### Tasks\n",
        "\n",
        "#### Task 2.1: Using `map` transformation\n",
        "Calculate engagement score for each post.\n",
        "- **Formula**: `engagement_score = (likes * 1) + (shares * 2)`\n",
        "- **Output**: `(user_id, post_content, engagement_score)`\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format: (\"U001\", \"Amazing sunset photography tips #photography #nature\", 69)\n",
        "```\n",
        "\n",
        "#### Task 2.2: Using `flatMap` transformation\n",
        "Extract all hashtags from posts to analyze trending topics.\n",
        "- **Input**: Post content containing hashtags\n",
        "- **Output**: Flattened RDD of individual hashtags (without # symbol)\n",
        "- **Note**: Extract words starting with '#', remove the '#' symbol, convert to lowercase\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output: [\"photography\", \"nature\", \"coding\", \"python\", ...]\n",
        "```\n",
        "\n",
        "#### Task 2.3: Using `sortBy` transformation\n",
        "Rank users by their average engagement score (descending order).\n",
        "- Calculate average engagement score per user\n",
        "- Sort users by average engagement in descending order\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format: [(\"U005\", avg_engagement), (\"U004\", avg_engagement), ...]\n",
        "```\n",
        "\n",
        "#### Task 2.4: Using `sortByKey` transformation\n",
        "Create daily engagement summary sorted by date.\n",
        "- **Input**: Calculate total engagement per day\n",
        "- **Output**: `(date, total_daily_engagement)` sorted by date\n",
        "- **Use**: `sortByKey()` for chronological sorting\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format: [(\"2024-01-15\", total_engagement), (\"2024-01-16\", total_engagement), ...]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Bonus Challenge\n",
        "\n",
        "Combine multiple transformations to solve this advanced problem:\n",
        "\n",
        "### Challenge: Top Trending Hashtags by Day\n",
        "Using the social media dataset, find the top 3 most frequently used hashtags for each day, sorted by date.\n",
        "\n",
        "**Requirements:**\n",
        "1. Use `flatMap` to extract hashtags with their dates\n",
        "2. Use `map` to count hashtag frequencies per day\n",
        "3. Use `sortByKey` to sort by date\n",
        "4. Use `sortBy` to rank hashtags within each day\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "# Expected output format:\n",
        "# [(\"2024-01-15\", [(\"photography\", count), (\"nature\", count), (\"food\", count)]),\n",
        "#  (\"2024-01-16\", [(\"coding\", count), (\"python\", count), (\"AI\", count)]), ...]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Submission Guidelines\n",
        "\n",
        "1. **Code Quality**: Write clean, well-commented code\n",
        "2. **Output**: Include sample outputs for each task\n",
        "3. **Explanation**: Briefly explain your approach for each transformation\n",
        "4. **Testing**: Test your code with the provided sample data\n",
        "5. **Performance**: Consider the efficiency of your transformations\n",
        "\n",
        "## Evaluation Criteria\n",
        "\n",
        "- **Correctness**: Solutions produce expected outputs\n",
        "- **Proper Use**: Correct application of specified transformations\n",
        "- **Code Style**: Clean, readable, and well-documented code\n",
        "- **Understanding**: Clear explanation of transformation logic\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Notes\n",
        "\n",
        "- Use only the specified transformations: `map`, `flatMap`, `sortBy`, `sortByKey`\n",
        "- Do not use actions like `collect()` or `count()` in transformation chains\n",
        "- Focus on the transformation logic rather than Spark configuration\n",
        "- Remember that RDD transformations are lazy and only executed when an action is called"
      ],
      "metadata": {
        "id": "3muVhoHvNmAW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVwObLWNNGYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLAtMAP opertions"
      ],
      "metadata": {
        "id": "rv8z37rYNP5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement 1 – Split Sentences into Words\n",
        "\n",
        "Question:\n",
        "You are given an RDD containing sentences:\n",
        "[\"I love learning PySpark\", \"PySpark is powerful\", \"Big data is amazing\"]\n",
        "Use flatMap() to split each sentence into words and print all individual words."
      ],
      "metadata": {
        "id": "5P7UhDYtNXk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I|love|learning|PySpark\", \"PySpark|is|powerful\", \"Big|data|is|amazing\"]"
      ],
      "metadata": {
        "id": "0fTCD28MNXDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}