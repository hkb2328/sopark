{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark Installation"
      ],
      "metadata": {
        "id": "UQ1xr3HcWyjW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ObwfQKXYWyPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iesMAUc_Wuv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "7af7380b-f842-4c79-875d-ac5b21f28e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:6 https://cli.github.com/packages stable InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,596 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,290 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,539 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,835 kB]\n",
            "Fetched 37.5 MB in 9s (3,983 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f518c2be720>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ca630b5c4bd7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evTe0KJcpSwH"
      },
      "source": [
        "# Agenda\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTT0NLIuqLpP"
      },
      "source": [
        "1. groupByKey\n",
        "2. aggregateByKey\n",
        "3. reduceByKey\n",
        "4. distinct\n",
        "5. filter\n",
        "\n",
        "Use Case :- 1\n",
        "\n",
        "Use Case :- 2\n",
        "\n",
        "Use Case :- 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg2iesrSqIr1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkM2pvexvAt"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgYUmSaXxwoL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSxBZ5BPxxm6"
      },
      "source": [
        "# ReduceByKey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-AxnMlKx5MK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWNCVrrbK-HU"
      },
      "source": [
        "reduceByKey is a powerful transformation in PySpark that operates on key-value pair RDDs (Resilient Distributed Datasets).\n",
        "\n",
        "(\"python\",1)\n",
        "(\"python\",1)\n",
        "(\"python\",1)\n",
        "(\"python\",1)\n",
        "(\"java\",1)\n",
        "(\"java\",1)\n",
        "\n",
        "(\"python\",4)\n",
        "(\"java\",2)\n",
        "\n",
        "\n",
        "The main purpose of reduceByKey is to aggregate the values of each key using a specified associative and commutative binary function, such as summation or multiplication.\n",
        "\n",
        "This Transformation groups the values associated with each key and applies a specified reduction to combine then into a single value per key\n",
        "\n",
        "How reduceByKey Works\n",
        "\n",
        "When you use reduceByKey, PySpark groups the values of each key together and then applies the reduction function to those values. This transformation is particularly useful for tasks like counting, summing, or aggregating data by a key.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Key-Value Pairs: reduceByKey operates on RDDs where each element is a tuple (key, value).\n",
        "\n",
        "Reduction Function: The function provided must be associative and commutative, meaning the order of operations doesn't change the result.\n",
        "\n",
        "Shuffling: reduceByKey triggers a shuffle operation because it needs to group all values associated with each key together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oAmh8Gu8LNbw",
        "outputId": "eea2992e-a56d-4609-db16-9d9310fdd362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "store1: 450\n",
            "store2: 500\n",
            "store3: 120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  store1 -> [250,200]\\n  store2 -> [200,300]\\n  store3 -> [120]\\n\\n  store1 -> 100+150 = 250\\n  store2 -> 200+300 = 500\\n  store3 -> 120\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sparkContext = spark.sparkContext\n",
        "\n",
        "# List of transactions (store, sales)\n",
        "transactions = [(\"store1\", 100), (\"store2\", 200), (\"store1\", 150), (\"store2\", 300), (\"store3\", 120),(\"store1\",200)]\n",
        "\n",
        "# Create an RDD from the transactions list\n",
        "transactions_rdd = sparkContext.parallelize(transactions)\n",
        "\n",
        "# Use reduceByKey to sum the sales for each store\n",
        "total_sales_rdd = transactions_rdd.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "\n",
        "\n",
        "# Collect the results\n",
        "total_sales = total_sales_rdd.collect()\n",
        "\n",
        "# Print the results\n",
        "for store, total in total_sales:\n",
        "  print(f\"{store}: {total}\")\n",
        "\n",
        "'''\n",
        "  store1 -> [250,200]\n",
        "  store2 -> [200,300]\n",
        "  store3 -> [120]\n",
        "\n",
        "  store1 -> 100+150 = 250\n",
        "  store2 -> 200+300 = 500\n",
        "  store3 -> 120\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaymYDdVx2SL"
      },
      "source": [
        "hello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqJKCoIEtJFd"
      },
      "source": [
        "# GroupByKey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjtpis4EuR_F"
      },
      "source": [
        "## GroupByKey\n",
        "\n",
        "Description:\n",
        "\n",
        "The groupByKey transformation in PySpark is used to group the values of a dataset (RDD) by a common key.\n",
        "\n",
        "When applied, it returns an RDD of pairs where the key is associated with an iterable of all the values that have the same key. Unlike reduceByKey, which performs aggregation, groupByKey simply groups the values and allows you to perform any operation on the grouped data later on.\n",
        "\n",
        "This transformation is particularly useful when you need to perform operations like aggregations or computations on data grouped by a key.\n",
        "\n",
        "Use Case:\n",
        "Imagine you are working with log data from a web server. The data consists of user IDs and the pages they visited. You want to analyze the data to find out which pages were visited by each user. Here, groupByKey can be used to group the pages by user ID.\n",
        "\n",
        "Example:-01\n",
        "\n",
        "# python list the users and pages visited\n",
        "\n",
        "data = [(\"user1\", \"page1\"), (\"user2\", \"page1\"), (\"user1\", \"page2\"), (\"user2\", \"page2\"), (\"user3\", \"page1\")]\n",
        "\n",
        "# Creata a RDD by using parallelize api\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Apply groupBykey to group the pages visited by each user\n",
        "\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "\n",
        "result = grouped_rdd.mapValues(list).collect()\n",
        "\n",
        "#  mapValues(list)\n",
        "\n",
        " This operation is used after groupByKey to convert the grouped values from an iterable (which is usually a generator in PySpark) to a list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhQZ3U4wuc5k",
        "outputId": "912c68b6-3617-41ef-a0a7-299b74b9179d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('user1', ['page1', 'page2']), ('user2', ['page1', 'page2']), ('user3', ['page1'])]\n"
          ]
        }
      ],
      "source": [
        "# python list the users and pages visited\n",
        "\n",
        "data = [(\"user1\", \"page1\"), (\"user2\", \"page1\"), (\"user1\", \"page2\"), (\"user2\", \"page2\"), (\"user3\", \"page1\")]\n",
        "\n",
        "# Creata a RDD by using parallelize api\n",
        "sparkContext = spark.sparkContext\n",
        "\n",
        "rdd = sparkContext.parallelize(data)\n",
        "\n",
        "# Apply groupBykey to group the pages visited by each user\n",
        "\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "\n",
        "result = grouped_rdd.mapValues(list).collect()\n",
        "print(result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwjIyto2YQd1",
        "outputId": "ea4c8a94-a367-4d13-cd39-28b4a40995a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('user1', <pyspark.resultiterable.ResultIterable object at 0x7fa623033740>), ('user2', <pyspark.resultiterable.ResultIterable object at 0x7fa623033770>), ('user3', <pyspark.resultiterable.ResultIterable object at 0x7fa623033860>)]\n"
          ]
        }
      ],
      "source": [
        "# python list the users and pages visited\n",
        "\n",
        "data = [(\"user1\", \"page1\"), (\"user2\", \"page1\"), (\"user1\", \"page2\"), (\"user2\", \"page2\"), (\"user3\", \"page1\")]\n",
        "\n",
        "# Creata a RDD by using parallelize api\n",
        "sparkContext = spark.sparkContext\n",
        "\n",
        "rdd = sparkContext.parallelize(data)\n",
        "\n",
        "# Apply groupBykey to group the pages visited by each user\n",
        "\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "\n",
        "result = grouped_rdd.collect()\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Du57dUdYQD0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWC5o2-UYPqf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep8vFleRwTQS",
        "outputId": "16cfe937-248b-4b3a-a7df-9a2c1c34ee33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('user1', <pyspark.resultiterable.ResultIterable at 0x7fa622f03b30>),\n",
              " ('user2', <pyspark.resultiterable.ResultIterable at 0x7fa622f03bc0>),\n",
              " ('user3', <pyspark.resultiterable.ResultIterable at 0x7fa622e861e0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#without mapValues\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "grouped_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcmIiyrwvu-N"
      },
      "source": [
        "Example 2:\n",
        "\n",
        "Grouping Sales Data by Product\n",
        "\n",
        "Consider an RDD containing sales data where each entry consists of a product ID and the amount sold:\n",
        "\n",
        "sales_data = [(\"product1\", 100), (\"product2\", 200), (\"product1\", 300), (\"product3\", 400), (\"product2\", 500)]\n",
        "\n",
        "rdd = sc.parallelize(sales_data)\n",
        "\n",
        "grouped_sales = rdd.groupByKey()\n",
        "\n",
        "result = grouped_sales.mapValues(list).collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytC8Z2ZuDEC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64f3_uPFwtCH",
        "outputId": "0ede3444-6529-43b7-faf4-c7433299c867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('product1', [100, 300]), ('product2', [200, 500]), ('product3', [400])]\n"
          ]
        }
      ],
      "source": [
        "sales_data = [(\"product1\", 100), (\"product2\", 200), (\"product1\", 300), (\"product3\", 400), (\"product2\", 500)]\n",
        "\n",
        "rdd = sparkContext.parallelize(sales_data)\n",
        "\n",
        "grouped_sales = rdd.groupByKey()\n",
        "\n",
        "result = grouped_sales.mapValues(list).collect()\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x = [1,2,3,4,5,6,7]"
      ],
      "metadata": {
        "id": "OMF2QtLBotep"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7pcCoRFxKVP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXacWz74xMws",
        "outputId": "9a3be762-8b16-4b0a-b107-70de09b7fe31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice: 85\n",
            "Bob: 78\n",
            "Charlie: 79\n"
          ]
        }
      ],
      "source": [
        "# example:-2\n",
        "\n",
        "\n",
        "\n",
        "# List of scores (student, score)\n",
        "scores = [(\"Alice\", 85), (\"Bob\", 78), (\"Alice\", 92), (\"Bob\", 88), (\"Charlie\", 79)]\n",
        "\n",
        "# Create an RDD from the scores list\n",
        "scores_rdd = sparkContext.parallelize(scores)\n",
        "\n",
        "# Use reduceByKey to find the maximum score for each student\n",
        "max_scores_rdd = scores_rdd.reduceByKey(lambda x, y: min(x, y))\n",
        "\n",
        "# Collect the results\n",
        "max_scores = max_scores_rdd.collect()\n",
        "\n",
        "# Print the results\n",
        "for student, max_score in max_scores:\n",
        "    print(f\"{student}: {max_score}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqs7mAljxPGU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9Fxeq2xP3_"
      },
      "source": [
        "# AggregrateBykey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ofvz3UIxiWz"
      },
      "source": [
        "Description:\n",
        "\n",
        "The aggregateByKey transformation in PySpark is used to aggregate values for each key using a specified sequence of operations.\n",
        "\n",
        " This transformation is more flexible than reduceByKey because it allows you to apply different operations when combining values within a partition and when combining results across partitions.\n",
        "\n",
        " The general syntax for aggregateByKey is:\n",
        "\n",
        " rdd.aggregateByKey(zeroValue)(seqOp, combOp)\n",
        "\n",
        " zeroValue: The initial value for the aggregation in each partition.\n",
        "\n",
        "seqOp: The function that combines a value from the RDD with the accumulator within a partition.\n",
        "\n",
        "combOp: The function that combines accumulators from different partitions.\n",
        "\n",
        "\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Consider a scenario where you want to calculate both the sum and count of values associated with each key in your dataset. This might be useful, for example, when calculating the average of values per key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qSJa2n5ybA5"
      },
      "source": [
        "Example 1: Calculating the Sum and Count of Values by Key\n",
        "\n",
        "Suppose you have the following RDD representing user scores in different games:\n",
        "\n",
        "\n",
        "data = [(\"user1\", 10), (\"user2\", 20), (\"user1\", 30), (\"user2\", 40), (\"user3\", 50)]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "You can use aggregateByKey to calculate the sum and count of scores for each user:\n",
        "\n",
        "\n",
        "zero_value = (0, 0)  # (sum, count)\n",
        "\n",
        "seqOp = lambda acc, value: (acc[0] + value, acc[1] + 1)\n",
        "\n",
        "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "\n",
        "result = rdd.aggregateByKey(zero_value)(seqOp, combOp).collect()\n",
        "\n",
        "Output:-\n",
        "\n",
        "[('user1', (40, 2)), ('user2', (60, 2)), ('user3', (50, 1))]\n",
        "\n",
        "Explanation of the Functions:\n",
        "\n",
        "zero_value = (0, 0): This initializes the sum and count for each key as (0, 0).\n",
        "\n",
        "seqOp = lambda acc, value: (acc[0] + value, acc[1] + 1): This function adds the current value to the running sum and increments the count by 1 for values within a partition.\n",
        "\n",
        "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]): This function combines the results from different partitions by summing the sums and counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjJm-POWxVc1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc[0] -> current sum so far\n",
        "acc[1] -> current count so far\n",
        "\n",
        "(10+ 30, 1 + 1)"
      ],
      "metadata": {
        "id": "ZxP4mu5TkSam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8isVZowhe7V",
        "outputId": "02ff2ef9-eb98-4cff-dab9-6a2a0c80c4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('user1', (40, 2)), ('user2', (60, 2)), ('user3', (50, 1))]\n"
          ]
        }
      ],
      "source": [
        "data = [(\"user1\", 10), (\"user2\", 20), (\"user1\", 30), (\"user2\", 40), (\"user3\", 50)]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# You can use aggregateByKey to calculate the sum and count of scores for each user:\n",
        "\n",
        "zero_value =  (0,0)# (sum, count)\n",
        "\n",
        "seqOp = lambda acc , value: (acc[0]+ value, acc[1] + 1)\n",
        "\n",
        "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "\n",
        "result = rdd.aggregateByKey(zero_value,seqOp,combOp).collect()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytUgaWmTjIKW"
      },
      "source": [
        "Steps in the Program:\n",
        "Initialization:\n",
        "\n",
        "zero_value = (0, 0): This is the initial value for each key (sum = 0, count = 0).\n",
        "Sequential Operation (seqOp):\n",
        "\n",
        "seqOp = lambda acc, value: (acc[0] + value, acc[1] + 1)\n",
        "\n",
        "This operation adds the current value to the accumulated sum and increments the count by 1 within each partition.\n",
        "\n",
        "Combining Operation (combOp):\n",
        "\n",
        "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "This operation merges the results from different partitions by adding the sums and counts.\n",
        "\n",
        "Processing:\n",
        "For user1:\n",
        "\n",
        "The first value is 10, so (0 + 10, 0 + 1) → (10, 1).\n",
        "The next value is 30, so (10 + 30, 1 + 1) → (40, 2).\n",
        "For user2:\n",
        "\n",
        "The first value is 20, so (0 + 20, 0 + 1) → (20, 1).\n",
        "The next value is 40, so (20 + 40, 1 + 1) → (60, 2).\n",
        "For user3:\n",
        "\n",
        "The only value is 50, so (0 + 50, 0 + 1) → (50, 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0xQ7HLiaea3"
      },
      "source": [
        "# Distinct and Filter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJZDf6umkvNB"
      },
      "source": [
        "The filter transformation in PySpark is used to create a new RDD (Resilient Distributed Dataset) or DataFrame containing only the elements that satisfy a given condition or predicate.\n",
        "\n",
        "It is a simple yet powerful operation that allows you to refine and process data by selecting only the rows or records that meet specific criteria.\n",
        "\n",
        "How filter Works\n",
        "\n",
        "RDDs: When applied to an RDD, filter takes a function as an argument. This function is applied to each element in the RDD, and only those elements for which the function returns True are included in the resulting RDD.\n",
        "\n",
        "DataFrames: In the context of DataFrames, filter (or where, which is an alias for filter) is used to apply conditions on columns. It returns a new DataFrame with rows that match the given condition.\n",
        "\n",
        "Syntax :-\n",
        "\n",
        "filtered_rdd = rdd.filter(lambda x: condition)\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] rdd = spark.sparkContext.parallelize(numbers)\n",
        "\n",
        "Filter out even numbers\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "print(filtered_rdd.collect()) # Output: [2, 4, 6, 8, 10]\n",
        "\n",
        "sc.stop()\n",
        "\n",
        "Common Use Cases\n",
        "\n",
        "Data Cleaning: Filtering out invalid or missing data (e.g., filter(lambda x: x is not None)).\n",
        "\n",
        "Data Analysis: Selecting specific subsets of data for analysis (e.g., filtering out customers who have made purchases above a certain threshold).\n",
        "\n",
        "Optimization: Reducing the size of data by filtering irrelevant information, which can lead to more efficient processing.\n",
        "\n",
        "Distinct Operation in PySpark\n",
        "The distinct transformation in PySpark is used to remove duplicate elements from an RDD (Resilient Distributed Dataset) or rows from a DataFrame, resulting in a dataset that contains only unique elements or rows. This operation is particularly useful when you need to ensure that your data contains no duplicates, which is a common requirement in data processing and analysis tasks.\n",
        "\n",
        "How distinct Works\n",
        "\n",
        "RDDs: When applied to an RDD, distinct will remove all duplicate elements, returning a new RDD containing only unique elements.\n",
        "\n",
        "Syntax:-\n",
        "\n",
        "distinct_rdd = rdd.distinct()\n",
        "\n",
        "Example -01\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2] rdd = sc.parallelize(numbers)\n",
        "\n",
        "Remove duplicates\n",
        "distinct_rdd = rdd.distinct()\n",
        "\n",
        "print(distinct_rdd.collect()) # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KlCx7jplZPP",
        "outputId": "21e4c781-94d1-47d3-9135-4cbfc0d1eb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ],
      "source": [
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = spark.sparkContext.parallelize(numbers)\n",
        "\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "print(filtered_rdd.collect()) # Output: [2, 4, 6, 8, 10]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numbers1 = [1, 2, 3, 4, 5,1,2]\n",
        "rdd1 = spark.sparkContext.parallelize(numbers1)\n",
        "\n",
        "distinct_rdd1 = rdd1.distinct()\n",
        "print(distinct_rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KWcBvOYq5sO",
        "outputId": "85a30125-a367-4a18-8fde-978ae134d160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 1, 3, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6kyR_TmmPd6"
      },
      "source": [
        "Summary ❎\n",
        "\n",
        "-> create RDD's\n",
        "-> map\n",
        "-> FlatMap\n",
        "-> sortbykey\n",
        "-> sortby\n",
        "-> reduceBykey\n",
        "-> groupbykey\n",
        "-> mapValues(list)\n",
        "-> aggregratebykey\n",
        "-> collect\n",
        "-> take\n",
        "-> distinct\n",
        "-> filter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7cTtnlDbEEC"
      },
      "source": [
        "# USE CASE - 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbXjuMDVbHr9"
      },
      "source": [
        "Website Log Data Analysis\n",
        "\n",
        "Objective:\n",
        "\n",
        "Identify the top 2 users with the most unique page visits.\n",
        "\n",
        "\n",
        "logs = [\n",
        "    (\"U001\", \"/home\", \"2024-08-25 10:00:00\"),\n",
        "    (\"U002\", \"/about\", \"2024-08-25 10:05:00\"),\n",
        "    (\"U001\", \"/products\", \"2024-08-25 10:10:00\"),\n",
        "    (\"U002\", \"/home\", \"2024-08-25 10:15:00\"),\n",
        "    (\"U003\", \"/products\", \"2024-08-25 10:20:00\"),\n",
        "    (\"U001\", \"/home\", \"2024-08-25 10:25:00\")\n",
        "]\n",
        "\n",
        "\n",
        "Expected Output:-\n",
        "\n",
        "[('U001', ['/home', '/products']), ('U002', ['/about', '/home'])]\n",
        "\n",
        "Operations to Perfrom:-\n",
        "\n",
        "✅Map\n",
        "\n",
        "✅Distinct\n",
        "\n",
        "✅GroupBykey and mapvalues\n",
        "\n",
        "✅SortBy\n",
        "\n",
        "✅take\n",
        "\n",
        "\n",
        "user_pages_rdd = rdd.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "distinct_user_pages = user_pages_rdd.distinct()\n",
        "\n",
        "grouped_pages_by_user = distinct_user_pages.groupByKey().mapValues(list)\n",
        "\n",
        "sorted_users_by_activity = grouped_pages_by_user.sortBy(lambda x: len(x[1]), ascending=False)\n",
        "\n",
        "\n",
        "top_active_users = sorted_users_by_activity.take(2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI7NDFqALyu7",
        "outputId": "d331b275-3c58-4533-c233-1811dff5fe79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('U001', ['/home', '/products']), ('U002', ['/about', '/home'])]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logs = [ (\"U001\", \"/home\", \"2024-08-25 10:00:00\"), (\"U002\", \"/about\", \"2024-08-25 10:05:00\"), (\"U001\", \"/products\", \"2024-08-25 10:10:00\"), (\"U002\", \"/home\", \"2024-08-25 10:15:00\"), (\"U003\", \"/products\", \"2024-08-25 10:20:00\"), (\"U001\", \"/home\", \"2024-08-25 10:25:00\") ]\n",
        "\n",
        "rdd= spark.sparkContext.parallelize(logs)\n",
        "rdd.collect()\n",
        "\n",
        "users_pages_rdd = rdd.map(lambda x: (x[0],x[1]))\n",
        "\n",
        "# users_pages_rdd.collect()\n",
        "\n",
        "distinct_rdd = users_pages_rdd.distinct()\n",
        "distinct_rdd.collect()\n",
        "grouped_rdd = distinct_rdd.groupByKey().mapValues(list)\n",
        "grouped_rdd.collect()\n",
        "sorted_rdd = grouped_rdd.sortBy(lambda x: len(x[1]),ascending=False)\n",
        "#sorted_rdd.collect()\n",
        "sorted_rdd.take(2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4y-AASZbK0Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRxIsxMbbLjY"
      },
      "source": [
        "# USE CASE - 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM3CChLQbOwC"
      },
      "source": [
        "Objective:\n",
        "\n",
        "Identify machines with more than one abnormal sensor reading (e.g., reading > 90) and sort them by the number of such readings.\n",
        "\n",
        "\n",
        "sensor_data = [\n",
        "    (\"M001\", 85, \"2024-08-25 10:00:00\"),\n",
        "    (\"M002\", 95, \"2024-08-25 10:05:00\"),\n",
        "    (\"M001\", 75, \"2024-08-25 10:10:00\"),\n",
        "    (\"M002\", 105, \"2024-08-25 10:15:00\"),\n",
        "    (\"M003\", 65, \"2024-08-25 10:20:00\"),\n",
        "    (\"M001\", 95, \"2024-08-25 10:25:00\"),\n",
        "    (\"M002\", 100, \"2024-08-25 10:30:00\")\n",
        "]\n",
        "\n",
        "\n",
        "Expected Output:\n",
        "\n",
        "[('M002', 3), ('M001', 1)]\n",
        "\n",
        "\n",
        "Operations to Perfrom:-\n",
        "\n",
        "✅Filter\n",
        "\n",
        "✅map\n",
        "\n",
        "✅ReduceBykey\n",
        "\n",
        "✅SortBy\n",
        "\n",
        "\n",
        "abnormal_readings = rdd.filter(lambda x: x[1] > 90)\n",
        "\n",
        "machine_abnormal_counts = abnormal_readings.map(lambda x: (x[0], 1))\n",
        "\n",
        "total_abnormal_counts = machine_abnormal_counts.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "sorted_abnormal_counts = total_abnormal_counts.sortBy(lambda x: x[1], ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k3SqOxxAovT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okFjjKFwbQ9I",
        "outputId": "4ed9b94a-e2d4-49f3-fb0c-073c29e100fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M001', 1), ('M002', 3)]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sensor_data = [ (\"M001\", 85, \"2024-08-25 10:00:00\"), (\"M002\", 95, \"2024-08-25 10:05:00\"), (\"M001\", 75, \"2024-08-25 10:10:00\"), (\"M002\", 105, \"2024-08-25 10:15:00\"), (\"M003\", 65, \"2024-08-25 10:20:00\"), (\"M001\", 95, \"2024-08-25 10:25:00\"), (\"M002\", 100, \"2024-08-25 10:30:00\") ]\n",
        "rdd = spark.sparkContext.parallelize(sensor_data)\n",
        "rdd.collect()\n",
        "rdd2= rdd.filter(lambda x: x[1] > 90)\n",
        "rdd3 = rdd2.map(lambda x : (x[0],1))\n",
        "rdd4 = rdd3.reduceByKey(lambda x,y: x+y)\n",
        "rdd5 = rdd4.sortBy(lambda x: x[1],ascending=True)\n",
        "rdd5.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJEYB5RbRjd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZSCo6D0bTRJ"
      },
      "source": [
        "# USE CASE - 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBEvHWabbVoH"
      },
      "source": [
        "E-commerce Transactions\n",
        "\n",
        "Objective:\n",
        "Calculate the total amount spent by each customer and sort the customers by their IDs.\n",
        "\n",
        "\n",
        "transactions = [\n",
        "    (\"C001\", \"Item1\", 100),\n",
        "    (\"C002\", \"Item2\", 150),\n",
        "    (\"C001\", \"Item3\", 200),\n",
        "    (\"C002\", \"Item1\", 300),\n",
        "    (\"C003\", \"Item2\", 250),\n",
        "    (\"C003\", \"Item3\", 100)\n",
        "]\n",
        "\n",
        "Expected Output\n",
        "\n",
        "[('C001', 300), ('C002', 450), ('C003', 350)]\n",
        "\n",
        "Operation to perform ->\n",
        "\n",
        "✅ Map\n",
        "\n",
        "✅ ReduceByKey\n",
        "\n",
        "✅ SortByKey\n",
        "\n",
        "mapped_rdd = rdd.map(lambda x: (x[0], x[2]))\n",
        "\n",
        "total_spent_per_customer = mapped_rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "sorted_customers = total_spent_per_customer.sortByKey()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bnhAThMbS0h",
        "outputId": "31730c75-482d-425a-d43c-f2b6723cdaef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('C001', 300), ('C002', 450), ('C003', 350)]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transactions = [ (\"C001\", \"Item1\", 100), (\"C002\", \"Item2\", 150), (\"C001\", \"Item3\", 200), (\"C002\", \"Item1\", 300), (\"C003\", \"Item2\", 250), (\"C003\", \"Item3\", 100) ]\n",
        "rdd = spark.sparkContext.parallelize(transactions)\n",
        "rdd2 = rdd.map(lambda x:(x[0],x[2]))\n",
        "rdd3 = rdd2.reduceByKey (lambda x,y: x+y)\n",
        "rdd4 = rdd3.sortByKey()\n",
        "rdd4.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qvFs0Uw-OLM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhw79kOLJ8D2"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiWSoeAwJ7Q6",
        "outputId": "ece6a65c-0159-4507-857c-16aa800e4b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "flight: 2\n",
            "delayed: 1\n",
            "customer: 1\n",
            "service: 1\n",
            "terrible.: 1\n",
            "good: 1\n",
            "seats: 1\n",
            "uncomfortable.: 1\n",
            "smooth: 1\n",
            "crew: 1\n",
            "flight.: 1\n",
            "working: 1\n",
            "properly.: 1\n",
            "food: 1\n",
            "were: 1\n",
            "friendly.: 1\n",
            "great: 1\n",
            "experience: 1\n",
            "entertainment: 1\n",
            "system: 1\n",
            "not: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# In this assignment, students will perform a word count on the provided dataset containing airline customer feedback. The goal is to count the number of occurrences of each word, ensuring that the word count is case-insensitive (i.e., \"Flight\" and \"flight\" should be treated as the same word). After counting the words, students should sort the results by the word counts in d\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import findspark\n",
        "import pyspark\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Sample airline feedback data\n",
        "feedback_data = [\n",
        "    \"The flight was delayed and the customer service was terrible.\",\n",
        "    \"The food was good but the seats were uncomfortable.\",\n",
        "    \"The flight was smooth and the crew was friendly.\",\n",
        "    \"I had a great experience on this flight.\",\n",
        "    \"The entertainment system was not working properly.\"\n",
        "]\n",
        "\n",
        "# Create an RDD from the feedback data\n",
        "rdd = sc.parallelize(feedback_data)\n",
        "stopwords = [\"the\", \"and\", \"was\", \"but\", \"on\", \"this\", \"had\", \"a\", \"in\", \"it\", \"i\"]\n",
        "\n",
        "# Broadcast the stopwords list to all nodes\n",
        "stopwords_broadcast = sc.broadcast(stopwords)\n",
        "# Split the sentences into words and convert to lowercase\n",
        "words = rdd.flatMap(lambda line: line.lower().split()) \\\n",
        "           .filter(lambda word: word not in stopwords_broadcast.value)\n",
        "\n",
        "# Map each word to a key-value pair (word, 1)\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "\n",
        "# Reduce by key to count word occurrences\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Sort the word counts in descending order\n",
        "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Collect and print the results\n",
        "for word, count in sorted_word_counts.collect():\n",
        "    print(f\"{word}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FldrxMpqKPP-"
      },
      "source": [
        "Assignment Description:\n",
        "\n",
        "In this assignment, students will perform a word count on the provided dataset containing airline customer feedback. The goal is to count the number of occurrences of each word, ensuring that the word count is case-insensitive (i.e., \"Flight\" and \"flight\" should be treated as the same word). After counting the words, students should sort the results by the word counts in descending order.\n",
        "\n",
        "Dataset : will be provided by the trainer\n",
        "\n",
        "Assignment Tasks:\n",
        "\n",
        "Mount the Dataset:\n",
        "\n",
        "Load the dataset (airline_feedback_short.txt) into a PySpark RDD\n",
        "\n",
        "Convert all the text to lowercase to ensure the word count is case-insensitive.\n",
        "\n",
        "Split the text into individual words.\n",
        "\n",
        "Task :- 1\n",
        "\n",
        "Word Count:\n",
        "\n",
        "Count how many times each word appears in the dataset. Sort the Word Count:\n",
        "\n",
        "Task :- 2\n",
        "\n",
        "Sort the words by their count in descending order. Capitalize Words:\n",
        "\n",
        "Task :- 3\n",
        "\n",
        "Apply capitalization to each word in the final output (i.e., make the first letter of each word uppercase). Save and Display Results:\n",
        "\n",
        "Task :- 4\n",
        "\n",
        "Save the final sorted word counts to a file. Display the top 20 most frequent words with their counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgDdaJMcyFz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKRJzFMJcv0v"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw6zKoUEKfFo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "L0xQ7HLiaea3",
        "n7cTtnlDbEEC",
        "HRxIsxMbbLjY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}